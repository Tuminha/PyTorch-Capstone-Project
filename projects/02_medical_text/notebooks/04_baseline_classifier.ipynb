{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Classifier\n",
    "\n",
    "## üéØ Concept Primer\n",
    "Simple baseline: mean-pooled word embeddings + Linear classifier.\n",
    "\n",
    "**Architecture:** Embedding ‚Üí Mean Pool ‚Üí Linear ‚Üí Output  \n",
    "**Expected:** Baseline performance to beat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Objectives\n",
    "1. Create Embedding layer\n",
    "2. Mean-pool embeddings\n",
    "3. Add Linear classifier\n",
    "4. Train baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Build Baseline Model\n",
    "\n",
    "### TODO 2: Define baseline classifier\n",
    "\n",
    "**Expected:** Class with embedding + mean pool + linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Define model\n",
    "class BaseLineClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, num_classes=13):\n",
    "        super(BaseLineClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # Shape: [batch=32, seq_len=512, embed_dim=100] and self.fc expects: [batch, embed_dim] = [32, 100]\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Train Baseline\n",
    "\n",
    "### TODO 3: Training loop\n",
    "\n",
    "**Expected:** Train for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df = pd.read_csv('../data/processed/specialty_taxonomy_v1.csv')\n",
    "\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "\n",
    "df['tokens'] = df['text_clean'].apply(lambda x: x.split())\n",
    "df['token_count'] = df['tokens'].apply(len)\n",
    "\n",
    "words = [word for tokens in df['tokens'] for word in tokens]\n",
    "vocab = Counter(words)\n",
    "word2idx = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"<UNK>\": 1,\n",
    "}\n",
    "\n",
    "VOCAB_SIZE = 15000  # Start with 10K, adjust later if needed\n",
    "\n",
    "# Get top 10K-2 words (subtract 2 for <PAD> and <UNK>)\n",
    "most_common_words = vocab.most_common(VOCAB_SIZE - 2)\n",
    "\n",
    "for idx, (word, count) in enumerate(most_common_words):\n",
    "    word2idx[word] = idx + 2  # Start at 2 (after <PAD> and <UNK>)\n",
    "\n",
    "def encode(tokens):\n",
    "    encoded = []\n",
    "    for token in tokens:\n",
    "        idx = word2idx.get(token, word2idx['<UNK>'])\n",
    "        encoded.append(idx)\n",
    "    return encoded\n",
    "\n",
    "df['encoded'] = df['tokens'].apply(encode)\n",
    "\n",
    "max_len = 512 # So we can use BERT\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    if len(seq) > max_len:\n",
    "        return seq[:max_len]\n",
    "    return seq + [0] * (max_len - len(seq))\n",
    "\n",
    "df['padded'] = df['encoded'].apply(lambda x: pad_sequence(x, max_len))\n",
    "\n",
    "#df[\"specialty\"] = df[\"specialty\"].str.lower()\n",
    "\n",
    "unique_specialities = df['specialty'].unique()\n",
    "label2idx = {label: idx for idx, label in enumerate(unique_specialities)}\n",
    "\n",
    "df['label_encoded'] = df['specialty'].map(label2idx)\n",
    "\n",
    "texts = df['padded'].tolist()\n",
    "labels = df['label_encoded'].tolist()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "dataset = TextDataset(texts, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, avg loss: 2.2914\n",
      "Epoch 2/10, avg loss: 2.0793\n",
      "Epoch 3/10, avg loss: 1.9176\n",
      "Epoch 4/10, avg loss: 1.7717\n",
      "Epoch 5/10, avg loss: 1.6532\n",
      "Epoch 6/10, avg loss: 1.5594\n",
      "Epoch 7/10, avg loss: 1.4802\n",
      "Epoch 8/10, avg loss: 1.4094\n",
      "Epoch 9/10, avg loss: 1.3458\n",
      "Epoch 10/10, avg loss: 1.2858\n",
      "\n",
      "==================================================\n",
      "BASELINE MODEL EVALUATION\n",
      "==================================================\n",
      "Accuracy:    0.6264\n",
      "F1 Weighted: 0.5918\n",
      "F1 Macro:    0.4801\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# TODO 3: Training\n",
    "baseline_model = BaseLineClassifier(\n",
    "    vocab_size=VOCAB_SIZE,    \n",
    "    embed_dim=100,       \n",
    "    num_classes=13       \n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = []\n",
    "    baseline_model.train()\n",
    "    for texts, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = baseline_model(texts)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "    \n",
    "    avg_loss = sum(total_loss) / len(total_loss)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "baseline_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts_batch, labels_batch in dataloader:\n",
    "        outputs = baseline_model(texts_batch)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        lbls = labels_batch.cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(lbls)\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"BASELINE MODEL EVALUATION\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy:    {accuracy:.4f}\")\n",
    "print(f\"F1 Weighted: {f1_weighted:.4f}\")\n",
    "print(f\"F1 Macro:    {f1_macro:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Reflection\n",
    "1. Baseline F1 score?\n",
    "2. Ready for transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your reflection:**\n",
    "\n",
    "*Write here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Summary\n",
    "‚úÖ Baseline trained  \n",
    "‚úÖ Performance recorded\n",
    "\n",
    "**Next:** `05_transformer_setup_train.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy ML",
   "language": "python",
   "name": "codeacademy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
