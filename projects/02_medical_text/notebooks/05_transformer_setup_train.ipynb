{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Setup & Training\n",
    "\n",
    "## ðŸŽ¯ Concept Primer\n",
    "Fine-tune TinyBERT or RoBERTa for medical text classification.\n",
    "\n",
    "**Expected:** Pretrained transformer fine-tuned on medical text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Objectives\n",
    "1. Load pretrained transformer\n",
    "2. Fine-tune on medical text\n",
    "3. Evaluate on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Import libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Load Transformer\n",
    "\n",
    "### TODO 2: Load model and tokenizer\n",
    "\n",
    "**Options:** TinyBERT, RoBERTa, DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 16407 samples\n",
      "Sampled dataset: 3281 samples (20% of each class)\n",
      "âš¡ Expected speedup: ~5x faster training!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/2c6x3jgj47j_fwl7231ts7m00000gn/T/ipykernel_40106/2824936658.py:9: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('specialty', group_keys=False).apply(lambda x: x.sample(frac=0.2, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# TODO 2: Load transformer\n",
    "model_name = 'dmis-lab/biobert-base-cased-v1.2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "df = pd.read_csv('../data/processed/specialty_taxonomy_v1.csv')\n",
    "\n",
    "# ðŸš€ SPEED HACK: Use 20% subset for faster experimentation\n",
    "print(f\"Original dataset: {len(df)} samples\")\n",
    "df = df.groupby('specialty', group_keys=False).apply(lambda x: x.sample(frac=0.2, random_state=42)).reset_index(drop=True)\n",
    "print(f\"Sampled dataset: {len(df)} samples (20% of each class)\")\n",
    "print(f\"âš¡ Expected speedup: ~5x faster training!\\n\")\n",
    "\n",
    "unique_specialities = df['specialty'].unique()\n",
    "label2idx = {label: idx for idx, label in enumerate(unique_specialities)}\n",
    "\n",
    "df['label_encoded'] = df['specialty'].map(label2idx)\n",
    "\n",
    "# Use RAW text (not cleaned!)\n",
    "texts = df['text'].tolist()  # â† Original text!\n",
    "labels = df['label_encoded'].tolist() \n",
    "\n",
    "# Split data (same as baseline)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    texts, \n",
    "    labels,  # You created this in Notebook 03!\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['label_encoded']\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Tokenize with BERT\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = BERTDataset(train_encodings, y_train)\n",
    "val_dataset = BERTDataset(val_encodings, y_val)\n",
    "test_dataset = BERTDataset(test_encodings, y_test)\n",
    "\n",
    "# DataLoaders (smaller batch size for memory!)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "class BioMedClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=13):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()  # Add loss function\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):  # â† Add labels parameter!\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.pooler_output\n",
    "        x = self.dropout(pooled)\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        # Calculate loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "        \n",
    "        # Return in same format as AutoModel\n",
    "        from collections import namedtuple\n",
    "        Output = namedtuple('Output', ['loss', 'logits'])\n",
    "        return Output(loss=loss, logits=logits)\n",
    "\n",
    "model = BioMedClassifier(num_classes=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Fine-tune\n",
    "\n",
    "### TODO 3: Training loop\n",
    "\n",
    "**Expected:** Fine-tune for 3-5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting training...\n",
      "Training samples: 1968\n",
      "Validation samples: 656\n",
      "Batches per epoch: 123\n",
      "Expected time per epoch (CPU): ~20-30 minutes\n",
      "\n",
      "============================================================\n",
      "ðŸ”„ Epoch 1/3\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# TODO 3: Fine-tune\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 3  # Reduced from 5 for faster experimentation\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "print(f\"ðŸš€ Starting training...\")\n",
    "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Expected time per epoch (CPU): ~20-30 minutes\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ðŸ”„ Epoch {epoch+1}/{n_epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # TRAINING\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # Model calculates loss automatically!\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        # Print progress every 100 batches\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            avg_loss = sum(train_loss[-100:]) / len(train_loss[-100:])\n",
    "            print(f\"  Batch [{batch_idx+1}/{len(train_loader)}] - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(f\"âœ… Training complete for epoch {epoch+1}\")\n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    print(f\"ðŸ“Š Running validation...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Epoch {epoch+1} Results:\")\n",
    "    print(f\"  Train Loss: {sum(train_loss)/len(train_loss):.4f}\")\n",
    "    print(f\"  Val Loss:   {sum(val_loss)/len(val_loss):.4f}\")\n",
    "    print(f\"  Val F1:     {val_f1:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), '../models/biobert_best.pth')\n",
    "        print(f\"  âœ… New best model saved! (F1: {val_f1:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤” Reflection\n",
    "1. Training time? GPU needed?\n",
    "2. Beat baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your reflection:**\n",
    "\n",
    "*Write here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Summary\n",
    "âœ… Transformer fine-tuned  \n",
    "âœ… Performance evaluated\n",
    "\n",
    "**Next:** `06_eval_and_error_analysis.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy ML",
   "language": "python",
   "name": "codeacademy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
