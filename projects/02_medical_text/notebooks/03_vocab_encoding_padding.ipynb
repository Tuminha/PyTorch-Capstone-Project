{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary, Encoding & Padding\n",
    "\n",
    "## üéØ Concept Primer\n",
    "Build vocabulary, encode words as integers, pad/truncate sequences to fixed length.\n",
    "\n",
    "**Expected:** Encoded sequences [N, max_len], vocab size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Objectives\n",
    "1. Build vocabulary from training data\n",
    "2. Encode tokens as integers\n",
    "3. Pad/truncate to fixed length\n",
    "4. Create DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Import libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Build Vocabulary\n",
    "\n",
    "### TODO 2: Create word-to-index mapping\n",
    "\n",
    "**Expected:** vocab_size, word2idx dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 15000\n",
      "Total unique words in corpus: 30644\n",
      "\n",
      "Top 10 most common words:\n",
      "[('the', 191081), ('of', 125100), ('and', 81675), ('a', 76784), ('in', 69925), ('to', 57829), ('is', 57530), ('or', 44890), ('are', 30766), ('that', 30339)]\n",
      "\n",
      "Least common words in vocab:\n",
      "[('antiquitin', 4), ('vrk', 4), ('correlates', 4), ('clonal', 4), ('upwards', 4)]\n"
     ]
    }
   ],
   "source": [
    "# TODO 2: Build vocab\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df = pd.read_csv('../data/processed/specialty_taxonomy_v1.csv')\n",
    "\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "\n",
    "df['tokens'] = df['text_clean'].apply(lambda x: x.split())\n",
    "df['token_count'] = df['tokens'].apply(len)\n",
    "\n",
    "words = [word for tokens in df['tokens'] for word in tokens]\n",
    "vocab = Counter(words)\n",
    "word2idx = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"<UNK>\": 1,\n",
    "}\n",
    "\n",
    "VOCAB_SIZE = 15000  # Start with 10K, adjust later if needed\n",
    "\n",
    "# Get top 10K-2 words (subtract 2 for <PAD> and <UNK>)\n",
    "most_common_words = vocab.most_common(VOCAB_SIZE - 2)\n",
    "\n",
    "for idx, (word, count) in enumerate(most_common_words):\n",
    "    word2idx[word] = idx + 2  # Start at 2 (after <PAD> and <UNK>)\n",
    "\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "print(f\"Total unique words in corpus: {len(vocab)}\")\n",
    "print(f\"\\nTop 10 most common words:\")\n",
    "print(vocab.most_common(10))\n",
    "print(f\"\\nLeast common words in vocab:\")\n",
    "print(most_common_words[-5:])  # Show the last 5 words you kept\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Encode Sequences\n",
    "\n",
    "### TODO 3: Convert tokens to integers\n",
    "\n",
    "**Expected:** Encoded lists of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text      specialty  \\\n",
      "0   0  Glaucoma is a group of diseases that can damag...  Ophthalmology   \n",
      "1   1  Nearly 2.7 million people have glaucoma, a lea...  Ophthalmology   \n",
      "2   2  Symptoms of Glaucoma  Glaucoma can develop in ...  Ophthalmology   \n",
      "3   3  Although open-angle glaucoma cannot be cured, ...  Ophthalmology   \n",
      "4   4  Glaucoma is a group of diseases that can damag...  Ophthalmology   \n",
      "\n",
      "                                          text_clean  \\\n",
      "0  glaucoma is a group of diseases that can damag...   \n",
      "1  nearly  million people have glaucoma a leading...   \n",
      "2  symptoms of glaucoma  glaucoma can develop in ...   \n",
      "3  although openangle glaucoma cannot be cured it...   \n",
      "4  glaucoma is a group of diseases that can damag...   \n",
      "\n",
      "                                              tokens  token_count  \\\n",
      "0  [glaucoma, is, a, group, of, diseases, that, c...          319   \n",
      "1  [nearly, million, people, have, glaucoma, a, l...          192   \n",
      "2  [symptoms, of, glaucoma, glaucoma, can, develo...          269   \n",
      "3  [although, openangle, glaucoma, cannot, be, cu...          314   \n",
      "4  [glaucoma, is, a, group, of, diseases, that, c...          111   \n",
      "\n",
      "                                             encoded  \n",
      "0  [1141, 8, 5, 154, 3, 276, 11, 18, 253, 2, 362,...  \n",
      "1  [2334, 1273, 17, 15, 1141, 5, 580, 53, 3, 1281...  \n",
      "2  [20, 3, 1141, 1141, 18, 166, 6, 52, 9, 213, 36...  \n",
      "3  [297, 6917, 1141, 455, 16, 3109, 47, 18, 65, 1...  \n",
      "4  [1141, 8, 5, 154, 3, 276, 11, 18, 253, 2, 362,...  \n",
      "Number of <UNK> tokens:\n",
      "7905\n"
     ]
    }
   ],
   "source": [
    "# TODO 3: Encode\n",
    "def encode(tokens):\n",
    "    encoded = []\n",
    "    for token in tokens:\n",
    "        idx = word2idx.get(token, word2idx['<UNK>'])\n",
    "        encoded.append(idx)\n",
    "    return encoded\n",
    "\n",
    "df['encoded'] = df['tokens'].apply(encode)\n",
    "\n",
    "print(df.head())\n",
    "print(\"Number of <UNK> tokens:\")\n",
    "print(df[\"encoded\"].apply(lambda seq: word2idx['<UNK>'] in seq).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìè Pad Sequences\n",
    "\n",
    "### TODO 4: Pad/truncate to fixed length\n",
    "\n",
    "**Expected:** All sequences same length (e.g., 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ophthalmology': 0, 'Endocrinology & Diabetes': 1, 'Oncology': 2, 'Infectious Diseases': 3, 'Cardiology & Vascular': 4, 'Nephrology & Urology': 5, 'Obstetrics & Gynecology': 6, 'Neurology & Neurosurgery': 7, 'Molecular Genetics & Mechanisms': 8, 'General Health & Prevention': 9, 'Pediatrics & Congenital Disorders': 10, 'Genetic & Chromosomal Syndromes': 11, 'Rare Genetic Disorders': 12}\n",
      "Any missing encodings? 0\n",
      "                   specialty  label_encoded\n",
      "0              Ophthalmology              0\n",
      "1              Ophthalmology              0\n",
      "2              Ophthalmology              0\n",
      "3              Ophthalmology              0\n",
      "4              Ophthalmology              0\n",
      "5              Ophthalmology              0\n",
      "6              Ophthalmology              0\n",
      "7              Ophthalmology              0\n",
      "8              Ophthalmology              0\n",
      "9              Ophthalmology              0\n",
      "10             Ophthalmology              0\n",
      "11             Ophthalmology              0\n",
      "12             Ophthalmology              0\n",
      "13             Ophthalmology              0\n",
      "14  Endocrinology & Diabetes              1\n",
      "15                  Oncology              2\n",
      "16  Endocrinology & Diabetes              1\n",
      "17                  Oncology              2\n",
      "18             Ophthalmology              0\n",
      "19             Ophthalmology              0\n",
      "20  Endocrinology & Diabetes              1\n",
      "21       Infectious Diseases              3\n",
      "22  Endocrinology & Diabetes              1\n",
      "23  Endocrinology & Diabetes              1\n",
      "24  Endocrinology & Diabetes              1\n",
      "Short: 512\n",
      "Long: 512\n",
      "Perfect: 512\n",
      "Short example: [1, 2, 3, 0, 0, 0, 0, 0, 0, 0]\n",
      "Short example: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# TODO 4: Pad\n",
    "max_len = 512 # So we can use BERT\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    if len(seq) > max_len:\n",
    "        return seq[:max_len]\n",
    "    return seq + [0] * (max_len - len(seq))\n",
    "\n",
    "df['padded'] = df['encoded'].apply(lambda x: pad_sequence(x, max_len))\n",
    "\n",
    "#df[\"specialty\"] = df[\"specialty\"].str.lower()\n",
    "\n",
    "unique_specialities = df['specialty'].unique()\n",
    "label2idx = {label: idx for idx, label in enumerate(unique_specialities)}\n",
    "\n",
    "print(label2idx)\n",
    "\n",
    "df['label_encoded'] = df['specialty'].map(label2idx)\n",
    "\n",
    "print(f\"Any missing encodings? {df['label_encoded'].isna().sum()}\")\n",
    "print(df[['specialty', 'label_encoded']].head(25))\n",
    "\n",
    "# Test cases:\n",
    "test_seq_short = [1, 2, 3]           # Too short\n",
    "test_seq_long = [i for i in range(600)]  # Too long\n",
    "test_seq_perfect = [i for i in range(512)]  # Perfect\n",
    "\n",
    "print(\"Short:\", len(pad_sequence(test_seq_short, max_len)))      # Should be 512\n",
    "print(\"Long:\", len(pad_sequence(test_seq_long, max_len)))        # Should be 512\n",
    "print(\"Perfect:\", len(pad_sequence(test_seq_perfect, max_len)))  # Should be 512\n",
    "\n",
    "# Check the values\n",
    "print(\"Short example:\", pad_sequence(test_seq_short, max_len)[:10])  # First 10\n",
    "print(\"Short example:\", pad_sequence(test_seq_short, max_len)[-10:]) # Last 10 (should be zeros!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Create DataLoader\n",
    "\n",
    "### TODO 5: Setup PyTorch DataLoader\n",
    "\n",
    "**Expected:** DataLoader with batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text dtype: torch.int64\n",
      "Label dtype: torch.int64\n",
      "Text shape: torch.Size([32, 512])\n",
      "Label shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# TODO 5: DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "texts = df['padded'].tolist()\n",
    "labels = df['label_encoded'].tolist()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "dataset = TextDataset(texts, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for batch_texts, batch_labels in dataloader:\n",
    "    print(f\"Text dtype: {batch_texts.dtype}\")      # Should be: torch.int64\n",
    "    print(f\"Label dtype: {batch_labels.dtype}\")    # Should be: torch.int64\n",
    "    print(f\"Text shape: {batch_texts.shape}\")      # Should be: [batch_size, 512]\n",
    "    print(f\"Label shape: {batch_labels.shape}\")    # Should be: [batch_size]\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Reflection\n",
    "1. Vocab size? Any OOV issues?\n",
    "2. Max length choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vocab size? Any OOV issues?\n",
    "\n",
    "**Vocabulary Size: 15,000 words**\n",
    "\n",
    "**Decision Rationale:**\n",
    "- Total unique words in corpus: 30,644\n",
    "- Kept top 15,000 most frequent words (49% coverage)\n",
    "- Excluded 15,644 rare words (51% of unique words)\n",
    "\n",
    "**Why 15,000?**\n",
    "- **Balance:** Large enough to capture medical terminology, small enough for efficient training\n",
    "- **Medical domain:** Needs more vocabulary than general text due to specialized terminology\n",
    "- **Diminishing returns:** Words beyond top 15K appear ‚â§4 times, offering little signal\n",
    "- **Model efficiency:** Smaller embedding matrix (15K √ó 768) vs. full vocab (30K √ó 768)\n",
    "\n",
    "**OOV (Out Of Vocabulary) Analysis:**\n",
    "- **48% of samples (7,905/16,407)** contain at least one <UNK> token\n",
    "- This is expected and acceptable because:\n",
    "  - Rare words (appearing 1-3 times) provide little learning signal\n",
    "  - Medical jargon variations often convey same meaning (e.g., \"hyperglycemia\" vs \"high blood sugar\")\n",
    "  - Model can infer meaning from context even with some <UNK> tokens\n",
    "  - Typos and formatting artifacts are naturally filtered out\n",
    "\n",
    "**Trade-offs Considered:**\n",
    "- Smaller vocab (5K-10K): Would miss important medical terms, higher UNK rate\n",
    "- Larger vocab (20K-25K): Marginal benefit (rare words), larger model, slower training\n",
    "- Full vocab (30K): Includes noise (typos, OCR errors), memory-intensive\n",
    "\n",
    "**Validation Strategy:**\n",
    "- Monitor model performance to see if 15K is sufficient\n",
    "- Can increase to 20K if UNK rate impacts accuracy\n",
    "- Least common words kept: appeared 4 times (e.g., 'antiquitin', 'vrk', 'correlates')\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Max length choice?\n",
    "\n",
    "**Max Sequence Length: 512 tokens**\n",
    "\n",
    "**Decision Rationale (from Notebook 02 analysis):**\n",
    "- **95th percentile:** 499 tokens ‚Üê Only 5% of texts are longer\n",
    "- **BERT's native max:** 512 tokens (no custom configuration needed)\n",
    "- **Data retention:** 95% of texts captured fully (15,587 samples)\n",
    "- **Truncation impact:** Only 5% truncated (820 samples) - acceptable loss\n",
    "\n",
    "**Why 512 is optimal:**\n",
    "1. ‚úÖ **Evidence-based:** Aligns with our token distribution analysis\n",
    "2. ‚úÖ **Standard:** BERT/transformer models trained with 512 max length\n",
    "3. ‚úÖ **Memory efficient:** 10x better than padding to max (4,183)\n",
    "4. ‚úÖ **Performance:** Sufficient context for specialty classification\n",
    "5. ‚úÖ **Practical:** Batch size of 32 √ó 512 = manageable memory usage\n",
    "\n",
    "**Alternative considered (not chosen):**\n",
    "- max_len = 256: Too short, would truncate 25% of data, lose information\n",
    "- max_len = 1024: Only capture 1% more data, double memory usage, slower training\n",
    "- max_len = 499 (exact 95th): Non-standard, minimal benefit vs. 512\n",
    "\n",
    "**Impact:**\n",
    "- **Padding overhead:** 75% of texts need padding (short texts)\n",
    "- **Truncation:** 5% of texts lose information (long medical explanations)\n",
    "- **Trade-off:** Accepted minor information loss for computational efficiency\n",
    "\n",
    "**Next steps:**\n",
    "- Monitor if truncated texts come from specific specialties (potential bias)\n",
    "- Consider attention masks to help model ignore padding tokens\n",
    "- Evaluate model performance to validate max_len choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Summary\n",
    "‚úÖ Vocab built  \n",
    "‚úÖ Sequences encoded  \n",
    "‚úÖ Padding applied  \n",
    "‚úÖ DataLoader ready\n",
    "\n",
    "**Next:** `04_baseline_classifier.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy ML",
   "language": "python",
   "name": "codeacademy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
