{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 00: Data-Driven Specialty Taxonomy\n\n## \ud83c\udfaf What This Notebook Does\n\n**Goal:** Transform 5,126 fine-grained `focus_area` labels into ~15-20 high-level medical specialties using embeddings + clustering.\n\n**Why?** The raw dataset has too many categories (5,126) with too few examples each (most < 10 samples). This makes classification impossible. By grouping related conditions into medical specialties (Oncology, Cardiology, etc.), we create a manageable multi-class problem.\n\n**Approach:** Bottom-up, data-driven taxonomy construction:\n1. **Embed** `focus_area` strings \u2192 semantic vectors (using BioBERT)\n2. **Cluster** vectors \u2192 discover natural groupings\n3. **Inspect** clusters \u2192 assign human-interpretable specialty labels\n4. **Validate** with zero-shot classification\n5. **Export** mapping + labeled dataframe\n\n**End Product:** A new column `df[\"specialty\"]` with ~15-20 balanced medical categories.\n\n---\n\n## \ud83d\udccb Key Phases\n\n1. \u2705 Project setup & data loading\n2. \u2705 Deduplicate focus_area catalog\n3. \u2705 Embed focus_areas with BioBERT\n4. \u2705 Dimensionality reduction (UMAP) for visualization\n5. \u2705 Cluster with HDBSCAN + k-means\n6. \u2705 Inspect clusters & assign specialty names\n7. \u2705 Validate with zero-shot classification\n8. \u2705 Merge back to full dataframe\n9. \u2705 Export final taxonomy\n\n---\n\n## \u2705 Acceptance Criteria\n\n- [ ] Created 15-20 medical specialties from 5,126 focus_areas\n- [ ] Each specialty has \u226550 samples (ideally \u2265100)\n- [ ] Clusters are semantically coherent (validated by inspection + zero-shot)\n- [ ] Exported: `df_with_specialty.parquet` + `focus_area_to_specialty.json`\n- [ ] Documented: Clustering params, model choices, manual decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Setup: Project Paths & Environment\n\n**TODO 1:** Configure paths and create output directories.\n\n**Hints:**\n- Point to your MedQuad CSV (likely in `../../../datasets/medquad.csv`)\n- Create an artifacts folder to save embeddings, clusters, and mappings\n- Use `pathlib.Path` for cross-platform compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Set up project paths & environment\n\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define paths\n# DATA_DIR = Path(\"../../../datasets\")\n# INPUT_PATH = DATA_DIR / \"medquad.csv\"\n# ARTIFACTS_DIR = Path(\"../artifacts/specialty_taxonomy\")\n\n# Create output directory\n# ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n\n# print(f\"\u2705 Input data: {INPUT_PATH}\")\n# print(f\"\u2705 Artifacts will be saved to: {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udcca Load Data & Basic EDA\n\n**TODO 2:** Load the MedQuad dataset and perform basic checks.\n\n**What to check:**\n- Dataset shape and columns\n- Presence of , , \n- Number of unique focus_areas\n- Top 20 most frequent focus_areas\n- Any missing values\n\n**Hints:**\n- Create a normalized version of  (strip whitespace, handle case)\n- Keep  and  for later sanity checks",
   "outputs": null,
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 2: Load data & basic EDA\n\n# Load dataframe\n# df = pd.read_csv(INPUT_PATH)\n\n# Basic info\n# print(\"Dataset shape:\", df.shape)\n# print(\"\\nColumns:\", list(df.columns))\n# print(\"\\nFirst few rows:\")\n# display(df.head())\n\n# Check for required columns\n# required_cols = ['focus_area', 'question', 'answer']\n# assert all(col in df.columns for col in required_cols), f\"Missing columns! Need: {required_cols}\"\n\n# Normalize focus_area for consistency\n# df['focus_area_norm'] = df['focus_area'].str.strip()\n\n# Remove any rows with missing focus_area\n# df = df.dropna(subset=['focus_area_norm'])\n# print(f\"\\n\u2705 After removing NaN focus_areas: {df.shape[0]} rows\")\n\n# Unique focus_areas\n# n_unique = df['focus_area_norm'].nunique()\n# print(f\"\\n\ud83d\udcca Number of unique focus_areas: {n_unique}\")\n\n# Top 20 most common\n# print(\"\\nTop 20 most frequent focus_areas:\")\n# print(df['focus_area_norm'].value_counts().head(20))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\uddc2\ufe0f Build Unique Focus_Area Catalog\n\n**TODO 3:** Create a catalog of unique focus_areas with their counts.\n\n**Why deduplicate?**\n- Clustering 5,126 unique strings is much faster than clustering 16,412 rows\n- Avoids over-weighting duplicates\n- Easier to inspect and label\n\n**Hints:**\n- Use `value_counts()` to get unique focus_areas with frequencies\n- Keep the counts - they'll help prioritize important clusters\n- Store as a list for embedding"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 3: Build unique focus_area catalog\n\n# Get unique focus_areas with counts\n# cat_counts = df['focus_area_norm'].value_counts()\n# catalog = cat_counts.index.tolist()\n\n# print(f\"\u2705 Catalog size: {len(catalog)} unique focus_areas\")\n# print(f\"   Total samples: {cat_counts.sum()}\")\n# print(f\"   Most common: {cat_counts.iloc[0]} ('{catalog[0]}')\")\n# print(f\"   Least common: {cat_counts.iloc[-1]} ('{catalog[-1]}')\")\n# print(f\"   Median count: {cat_counts.median():.0f}\")\n\n# Distribution of counts\n# plt.figure(figsize=(10, 4))\n# plt.subplot(1, 2, 1)\n# plt.hist(cat_counts, bins=50, edgecolor='black')\n# plt.xlabel('Number of samples per focus_area')\n# plt.ylabel('Frequency')\n# plt.title('Distribution of Focus_Area Counts')\n\n# plt.subplot(1, 2, 2)\n# plt.hist(np.log10(cat_counts + 1), bins=50, edgecolor='black')\n# plt.xlabel('Log10(count + 1)')\n# plt.ylabel('Frequency')\n# plt.title('Log-scale Distribution')\n# plt.tight_layout()\n# plt.savefig(ARTIFACTS_DIR / 'focus_area_distribution.png', dpi=150)\n# plt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83e\uddec Embed Focus_Areas with BioBERT\n\n**TODO 4:** Convert focus_area strings to semantic vectors.\n\n**Model Choice:**\n- **Recommended:** `\"pritamdeka/BioBERT-mnli-snli-scitail-mednli\"` (medical domain)\n- **Alternative:** `\"all-mpnet-base-v2\"` (general, strong performance)\n\n**Why embeddings?**\n- Capture semantic meaning (\"Breast Cancer\" \u2248 \"Lung Cancer\" \u2248 \"Prostate Cancer\")\n- Enable clustering by similarity\n- Standard NLP technique\n\n**Hints:**\n- Use `sentence-transformers` library\n- Batch encode for efficiency\n- **SAVE embeddings to disk** - expensive to recompute!\n- Embeddings shape: `(n_catalog, embedding_dim)` e.g., `(5126, 768)`"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 4: Embed focus_areas with BioBERT\n\n# from sentence_transformers import SentenceTransformer\n\n# Initialize model\n# print(\"Loading embedding model...\")\n# model = SentenceTransformer(\"pritamdeka/BioBERT-mnli-snli-scitail-mednli\")\n# # Alternative: model = SentenceTransformer(\"all-mpnet-base-v2\")\n\n# Encode catalog (this may take a few minutes)\n# print(f\"Encoding {len(catalog)} focus_areas...\")\n# embeddings = model.encode(\n#     catalog,\n#     batch_size=64,\n#     convert_to_numpy=True,\n#     show_progress_bar=True\n# )\n\n# print(f\"\u2705 Embeddings shape: {embeddings.shape}\")\n\n# Save to disk\n# np.save(ARTIFACTS_DIR / \"catalog_embeddings.npy\", embeddings)\n# print(f\"\u2705 Saved embeddings to {ARTIFACTS_DIR / 'catalog_embeddings.npy'}\")\n\n# # Load embeddings (for future runs)\n# # embeddings = np.load(ARTIFACTS_DIR / \"catalog_embeddings.npy\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udcc9 Dimensionality Reduction: UMAP for Visualization\n\n**TODO 5:** Reduce embeddings to 2D for visualization (optional but highly recommended).\n\n**Why UMAP?**\n- Visualize cluster structure before clustering\n- Spot natural groupings and outliers\n- Helps tune clustering hyperparameters\n- Beautiful plots!\n\n**Hints:**\n- Use `metric='cosine'` for text embeddings\n- `n_neighbors=15-30` balances local/global structure\n- Save 2D coordinates for later plotting"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 5: UMAP dimensionality reduction\n\n# from umap import UMAP\n\n# Fit UMAP\n# print(\"Running UMAP (this may take a few minutes)...\")\n# umap_model = UMAP(\n#     n_components=2,\n#     metric='cosine',\n#     n_neighbors=15,\n#     min_dist=0.1,\n#     random_state=42\n# )\n# coords_2d = umap_model.fit_transform(embeddings)\n\n# print(f\"\u2705 UMAP coordinates shape: {coords_2d.shape}\")\n\n# Save coordinates\n# np.save(ARTIFACTS_DIR / \"coords_2d.npy\", coords_2d)\n\n# Quick visualization (before clustering)\n# plt.figure(figsize=(12, 8))\n# plt.scatter(coords_2d[:, 0], coords_2d[:, 1], s=5, alpha=0.5)\n# plt.title('UMAP Projection of Focus_Area Embeddings')\n# plt.xlabel('UMAP 1')\n# plt.ylabel('UMAP 2')\n# plt.savefig(ARTIFACTS_DIR / 'umap_before_clustering.png', dpi=150)\n# plt.show()\n\n# print(\"\ud83d\udca1 Look for natural clusters/islands in the plot above!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83c\udfaf Clustering Experiment #1: HDBSCAN\n\n**TODO 6A:** Cluster with HDBSCAN (automatically determines number of clusters).\n\n**HDBSCAN Advantages:**\n- No need to specify number of clusters\n- Finds dense regions\n- Labels noise points as -1\n- Good for irregular cluster shapes\n\n**Hyperparameters to tune:**\n- `min_cluster_size`: Minimum samples per cluster (try 20-60)\n- `min_samples`: Affects how conservative clustering is\n- `metric`: 'euclidean' works well with normalized embeddings\n\n**Hints:**\n- Normalize embeddings first (L2 norm) if using euclidean metric\n- Start with `min_cluster_size=40`\n- Some points will be labeled as noise (-1) - that's OK!"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 6A: HDBSCAN clustering\n\n# import hdbscan\n# from sklearn.preprocessing import normalize\n\n# Normalize embeddings for better clustering\n# embeddings_norm = normalize(embeddings, norm='l2', axis=1)\n\n# Run HDBSCAN\n# print(\"Running HDBSCAN...\")\n# clusterer_hdb = hdbscan.HDBSCAN(\n#     min_cluster_size=40,\n#     min_samples=5,\n#     metric='euclidean',\n#     cluster_selection_method='eom'\n# )\n# labels_hdb = clusterer_hdb.fit_predict(embeddings_norm)\n\n# Analyze results\n# n_clusters_hdb = len(set(labels_hdb)) - (1 if -1 in labels_hdb else 0)\n# n_noise = list(labels_hdb).count(-1)\n\n# print(f\"\\n\u2705 HDBSCAN Results:\")\n# print(f\"   Number of clusters: {n_clusters_hdb}\")\n# print(f\"   Noise points: {n_noise} ({100*n_noise/len(labels_hdb):.1f}%)\")\n\n# Save labels\n# np.save(ARTIFACTS_DIR / \"labels_hdbscan.npy\", labels_hdb)\n\n# Visualize clusters\n# plt.figure(figsize=(12, 8))\n# scatter = plt.scatter(coords_2d[:, 0], coords_2d[:, 1], \n#                       c=labels_hdb, s=5, alpha=0.5, cmap='tab20')\n# plt.colorbar(scatter, label='Cluster ID')\n# plt.title(f'HDBSCAN Clusters (n={n_clusters_hdb})')\n# plt.xlabel('UMAP 1')\n# plt.ylabel('UMAP 2')\n# plt.savefig(ARTIFACTS_DIR / 'hdbscan_clusters.png', dpi=150)\n# plt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83c\udfaf Clustering Experiment #2: K-Means\n\n**TODO 6B:** Cluster with k-means (you specify number of clusters).\n\n**K-Means Advantages:**\n- Simple and fast\n- Forces assignment (no noise points)\n- You control number of clusters\n\n**Choosing K:**\n- Start with 30-50 clusters (fine-grained)\n- Plan to merge similar clusters later\n- Medical specialties typically: 15-25 major categories\n\n**Hints:**\n- Use `n_init='auto'` for automatic initialization\n- Compare with HDBSCAN results\n- You'll likely use k-means results (easier to label)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 6B: K-Means clustering\n\n# from sklearn.cluster import KMeans\n\n# Choose K (start with 40)\n# K = 40\n\n# Run k-means\n# print(f\"Running k-means with K={K}...\")\n# km = KMeans(\n#     n_clusters=K,\n#     random_state=42,\n#     n_init='auto'\n# )\n# labels_km = km.fit_predict(embeddings_norm)\n\n# print(f\"\\n\u2705 K-Means Results:\")\n# print(f\"   Number of clusters: {K}\")\n# print(f\"   Cluster sizes: min={np.bincount(labels_km).min()}, max={np.bincount(labels_km).max()}\")\n\n# Save labels\n# np.save(ARTIFACTS_DIR / \"labels_kmeans.npy\", labels_km)\n\n# Visualize clusters\n# plt.figure(figsize=(12, 8))\n# scatter = plt.scatter(coords_2d[:, 0], coords_2d[:, 1], \n#                       c=labels_km, s=5, alpha=0.5, cmap='tab20')\n# plt.colorbar(scatter, label='Cluster ID')\n# plt.title(f'K-Means Clusters (K={K})')\n# plt.xlabel('UMAP 1')\n# plt.ylabel('UMAP 2')\n# plt.savefig(ARTIFACTS_DIR / 'kmeans_clusters.png', dpi=150)\n# plt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udd0d Inspect Clusters: Build Summary Table\n\n**TODO 7:** Create a DataFrame with cluster assignments and inspect each cluster.\n\n**What to inspect:**\n- Top 50 most frequent focus_areas per cluster\n- Total samples per cluster\n- Look for common themes (cancer types, heart conditions, eye diseases, etc.)\n\n**Decision:** Choose which clustering to use (HDBSCAN or k-means)\n- **Recommendation:** Start with k-means (easier to assign all points)\n\n**Hints:**\n- Create a catalog DataFrame: `[focus_area, count, cluster_label, coords_2d]`\n- Loop through clusters and display top members\n- Save per-cluster CSVs for detailed review"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 7: Build cluster summary table\n\n# Choose which clustering to use (k-means recommended)\n# cluster_labels = labels_km  # or labels_hdb\n\n# Build catalog DataFrame\n# catalog_df = pd.DataFrame({\n#     'focus_area': catalog,\n#     'count': [cat_counts[fa] for fa in catalog],\n#     'cluster': cluster_labels,\n#     'umap_x': coords_2d[:, 0],\n#     'umap_y': coords_2d[:, 1]\n# })\n\n# print(f\"\u2705 Catalog DataFrame shape: {catalog_df.shape}\")\n# display(catalog_df.head())\n\n# Save full catalog with clusters\n# catalog_df.to_csv(ARTIFACTS_DIR / 'catalog_with_clusters.csv', index=False)\n\n# Print summary per cluster\n# print(\"\\n\ud83d\udcca Cluster Summaries:\")\n# for c in sorted(catalog_df['cluster'].unique()):\n#     if c == -1:  # Skip noise if using HDBSCAN\n#         continue\n#     cluster_data = catalog_df[catalog_df['cluster'] == c].sort_values('count', ascending=False)\n#     total_samples = cluster_data['count'].sum()\n#     print(f\"\\n{'='*80}\")\n#     print(f\"Cluster {c}: {len(cluster_data)} unique focus_areas, {total_samples} total samples\")\n#     print(f\"{'='*80}\")\n#     print(cluster_data[['focus_area', 'count']].head(20).to_string(index=False))\n#     print()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83c\udff7\ufe0f Manual Labeling: Assign Specialty Names to Clusters\n\n**TODO 8:** Create a mapping from `cluster_id` \u2192 `specialty_name`.\n\n**Common Medical Specialties:**\n- **Oncology** - All cancers (breast, lung, prostate, colorectal, skin, etc.)\n- **Cardiology** - Heart conditions (heart failure, heart attack, cholesterol, blood pressure)\n- **Neurology** - Brain conditions (stroke, Alzheimer's, Parkinson's, headaches)\n- **Ophthalmology** - Eye conditions (glaucoma, macular degeneration, cataracts)\n- **Endocrinology** - Hormone/metabolic (diabetes, thyroid, obesity)\n- **Pulmonology** - Lung conditions (COPD, asthma, lung cancer)\n- **Gastroenterology** - Digestive system\n- **Nephrology** - Kidney conditions\n- **Dermatology** - Skin conditions\n- **Orthopedics** - Bone/joint conditions\n- **Infectious Disease** - Infections (HIV, flu, etc.)\n- **Mental Health** - Psychology/psychiatry\n- **General Medicine** - Primary care, preventive health\n- **Other** - Miscellaneous/uncategorized\n\n**Strategy:**\n1. Start with obvious clusters (e.g., cluster with all cancers \u2192 \"Oncology\")\n2. Merge similar small clusters (e.g., heart-related clusters \u2192 \"Cardiology\")\n3. Create \"Other\" for truly mixed clusters\n4. Aim for 15-20 final specialties\n\n**Hints:**\n- Iterate! This is not one-and-done\n- Save mapping to JSON for reproducibility\n- Document your decisions"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 8: Create cluster \u2192 specialty mapping\n\n# Manual mapping based on cluster inspection above\n# cluster_to_specialty = {\n#     0: \"Oncology\",\n#     1: \"Cardiology\",\n#     2: \"Neurology\",\n#     3: \"Ophthalmology\",\n#     4: \"Endocrinology\",\n#     5: \"Pulmonology\",\n#     6: \"Gastroenterology\",\n#     7: \"Nephrology\",\n#     8: \"Dermatology\",\n#     9: \"Orthopedics\",\n#     10: \"Infectious Disease\",\n#     11: \"Mental Health\",\n#     12: \"General Medicine\",\n#     # ... continue for all clusters ...\n#     -1: \"Other\"  # For noise (if using HDBSCAN)\n# }\n\n# TODO: Fill in the mapping based on your cluster inspection!\n# Inspect the output from TODO 7 above and assign meaningful names\n\n# Save mapping\n# with open(ARTIFACTS_DIR / 'cluster_to_specialty.json', 'w') as f:\n#     json.dump(cluster_to_specialty, f, indent=2)\n\n# print(f\"\u2705 Created mapping for {len(cluster_to_specialty)} clusters\")\n# print(f\"   Unique specialties: {len(set(cluster_to_specialty.values()))}\")\n# print(\"\\nSpecialty distribution:\")\n# from collections import Counter\n# print(Counter(cluster_to_specialty.values()))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udd04 Apply Specialty Labels to Catalog\n\n**TODO 9:** Map cluster labels to specialty names in the catalog DataFrame.\n\n**Steps:**\n1. Add `specialty` column using `cluster_to_specialty` mapping\n2. Handle any unmapped clusters \u2192 \"Other\"\n3. Compute per-specialty sample counts\n4. Check for class balance\n\n**Goal:** Ensure each specialty has \u226550 samples (ideally \u2265100)\n\n**Hints:**\n- Use `df['cluster'].map(cluster_to_specialty)`\n- Fill missing values with \"Other\"\n- Visualize specialty distribution"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 9: Apply specialty labels to catalog\n\n# Map clusters to specialties\n# catalog_df['specialty'] = catalog_df['cluster'].map(cluster_to_specialty)\n\n# Fill any missing with \"Other\"\n# catalog_df['specialty'] = catalog_df['specialty'].fillna('Other')\n\n# Compute per-specialty stats\n# specialty_counts = catalog_df.groupby('specialty').agg({\n#     'focus_area': 'count',  # Number of unique focus_areas\n#     'count': 'sum'          # Total samples\n# }).rename(columns={'focus_area': 'n_unique_focus_areas', 'count': 'n_samples'})\n# specialty_counts = specialty_counts.sort_values('n_samples', ascending=False)\n\n# print(\"\\n\ud83d\udcca Specialty Distribution:\")\n# print(specialty_counts)\n\n# Check minimum samples\n# min_samples = specialty_counts['n_samples'].min()\n# print(f\"\\n\u2705 Minimum samples per specialty: {min_samples}\")\n# if min_samples < 50:\n#     print(\"\u26a0\ufe0f  Warning: Some specialties have <50 samples. Consider merging!\")\n\n# Save catalog with specialties\n# catalog_df.to_csv(ARTIFACTS_DIR / 'catalog_with_specialty.csv', index=False)\n\n# Visualize distribution\n# plt.figure(figsize=(12, 6))\n# specialty_counts['n_samples'].plot(kind='bar')\n# plt.title('Samples per Specialty')\n# plt.xlabel('Specialty')\n# plt.ylabel('Number of Samples')\n# plt.xticks(rotation=45, ha='right')\n# plt.axhline(y=100, color='r', linestyle='--', label='Target: 100 samples')\n# plt.legend()\n# plt.tight_layout()\n# plt.savefig(ARTIFACTS_DIR / 'specialty_distribution.png', dpi=150)\n# plt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udd04 Merge Specialty Mapping Back to Full DataFrame\n\n**TODO 10:** Expand the focus_area \u2192 specialty mapping to all 16k+ rows.\n\n**Steps:**\n1. Merge `df` with `catalog_df` on `focus_area_norm`\n2. Every row now has a `specialty` column\n3. Verify no missing specialties (or fill with \"Other\")\n4. Save the enriched dataframe\n\n**Hints:**\n- Use a left join to keep all original rows\n- Check for any NaN specialties\n- Save as parquet for efficiency"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 10: Merge specialty labels back to full dataframe\n\n# Select columns to merge\n# mapping_df = catalog_df[['focus_area', 'specialty', 'cluster']]\n\n# Merge\n# df = df.merge(\n#     mapping_df,\n#     left_on='focus_area_norm',\n#     right_on='focus_area',\n#     how='left',\n#     suffixes=('', '_mapped')\n# )\n\n# # Drop duplicate focus_area column if created\n# if 'focus_area_mapped' in df.columns:\n#     df = df.drop(columns=['focus_area_mapped'])\n\n# # Check for missing specialties\n# missing_specialty = df['specialty'].isna().sum()\n# if missing_specialty > 0:\n#     print(f\"\u26a0\ufe0f  Warning: {missing_specialty} rows missing specialty. Filling with 'Other'.\")\n#     df['specialty'] = df['specialty'].fillna('Other')\n\n# print(f\"\\n\u2705 Full dataframe shape: {df.shape}\")\n# print(f\"   Columns: {list(df.columns)}\")\n\n# # Verify specialty distribution\n# print(\"\\n\ud83d\udcca Final specialty distribution:\")\n# print(df['specialty'].value_counts())\n\n# # Save enriched dataframe\n# df.to_parquet(ARTIFACTS_DIR / 'df_with_specialty.parquet', index=False)\n# print(f\"\\n\u2705 Saved enriched dataframe to {ARTIFACTS_DIR / 'df_with_specialty.parquet'}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \u2705 Validate with Zero-Shot Classification (Optional but Recommended)\n\n**TODO 11:** Use zero-shot classification as a second opinion.\n\n**Why zero-shot validation?**\n- Independent check on your manual labeling\n- Flags potential misclassifications\n- Identifies ambiguous cases\n\n**Approach:**\n1. Sample 200-500 focus_areas\n2. Run zero-shot classifier with your specialty list as candidates\n3. Compare `specialty` (from clustering) vs. zero-shot prediction\n4. Inspect disagreements\n\n**Hints:**\n- Use `microsoft/deberta-v3-large-mnli` (strong zero-shot model)\n- Focus on high-count focus_areas (more important to get right)\n- This is slow - run on a sample!"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 11: Zero-shot validation (optional)\n\n# from transformers import pipeline\n\n# # Initialize zero-shot classifier\n# print(\"Loading zero-shot classifier (this may take a minute)...\")\n# zero_shot_clf = pipeline(\n#     \"zero-shot-classification\",\n#     model=\"microsoft/deberta-v3-large-mnli\"\n# )\n\n# # Get unique specialties\n# candidate_labels = sorted(catalog_df['specialty'].unique())\n# print(f\"\\nCandidate specialties: {candidate_labels}\")\n\n# # Sample focus_areas for validation (prioritize high-count ones)\n# sample_df = catalog_df.nlargest(200, 'count').copy()\n\n# # Run zero-shot classification\n# print(f\"\\nRunning zero-shot on {len(sample_df)} focus_areas...\")\n# zero_shot_predictions = []\n# for idx, row in sample_df.iterrows():\n#     result = zero_shot_clf(\n#         row['focus_area'],\n#         candidate_labels=candidate_labels,\n#         multi_label=False\n#     )\n#     zero_shot_predictions.append(result['labels'][0])  # Top prediction\n\n# sample_df['zero_shot_specialty'] = zero_shot_predictions\n\n# # Compare with cluster-based specialty\n# sample_df['agreement'] = sample_df['specialty'] == sample_df['zero_shot_specialty']\n# agreement_rate = sample_df['agreement'].mean()\n\n# print(f\"\\n\u2705 Agreement rate: {agreement_rate:.1%}\")\n\n# # Show disagreements\n# disagreements = sample_df[~sample_df['agreement']].sort_values('count', ascending=False)\n# print(f\"\\n\u26a0\ufe0f  Disagreements ({len(disagreements)}):\")\n# print(disagreements[['focus_area', 'count', 'specialty', 'zero_shot_specialty']].head(20))\n\n# # Save validation results\n# sample_df.to_csv(ARTIFACTS_DIR / 'zero_shot_validation.csv', index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udcca Final Quality Checks & Balance Analysis\n\n**TODO 12:** Assess the final specialty distribution and decide on any final adjustments.\n\n**Quality Checks:**\n1. **Minimum samples:** Every specialty should have \u226550 samples\n2. **Class balance:** Largest/smallest ratio should be <50:1\n3. **Number of classes:** Should be 15-25 (too few = loss of granularity, too many = data scarcity)\n\n**If problems exist:**\n- Merge rare specialties into \"Other\" or related specialties\n- Split very large specialties if semantically distinct\n\n**Hints:**\n- Create a threshold function to group rare classes\n- Document any merging decisions"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 12: Final quality checks and balance\n\n# Compute final stats\n# specialty_final = df['specialty'].value_counts().sort_values(ascending=False)\n# n_specialties = len(specialty_final)\n# min_samples = specialty_final.min()\n# max_samples = specialty_final.max()\n# imbalance_ratio = max_samples / min_samples\n\n# print(\"\ud83d\udcca Final Taxonomy Stats:\")\n# print(f\"   Number of specialties: {n_specialties}\")\n# print(f\"   Total samples: {len(df)}\")\n# print(f\"   Min samples per specialty: {min_samples}\")\n# print(f\"   Max samples per specialty: {max_samples}\")\n# print(f\"   Imbalance ratio: {imbalance_ratio:.1f}:1\")\n\n# print(\"\\nPer-specialty counts:\")\n# print(specialty_final)\n\n# # Check acceptance criteria\n# print(\"\\n\u2705 Acceptance Criteria Check:\")\n# print(f\"   \u2713 Min samples \u226550? {min_samples >= 50}\")\n# print(f\"   \u2713 Number of classes 15-25? {15 <= n_specialties <= 25}\")\n# print(f\"   \u2713 Imbalance ratio <50:1? {imbalance_ratio < 50}\")\n\n# # Optional: Apply minimum sample threshold\n# MIN_SAMPLES = 50\n# rare_specialties = specialty_final[specialty_final < MIN_SAMPLES].index.tolist()\n# if rare_specialties:\n#     print(f\"\\n\u26a0\ufe0f  Rare specialties (<{MIN_SAMPLES} samples): {rare_specialties}\")\n#     print(\"   Consider merging these into 'Other' or related specialties.\")\n#     # df.loc[df['specialty'].isin(rare_specialties), 'specialty'] = 'Other'\n\n# # Visualize final distribution\n# plt.figure(figsize=(14, 6))\n# specialty_final.plot(kind='barh')\n# plt.xlabel('Number of Samples')\n# plt.ylabel('Specialty')\n# plt.title(f'Final Specialty Distribution (n={n_specialties} classes)')\n# plt.axvline(x=100, color='g', linestyle='--', label='Target: 100+')\n# plt.axvline(x=50, color='r', linestyle='--', label='Minimum: 50')\n# plt.legend()\n# plt.tight_layout()\n# plt.savefig(ARTIFACTS_DIR / 'final_specialty_distribution.png', dpi=150)\n# plt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udcbe Export Final Taxonomy & Metadata\n\n**TODO 13:** Save all artifacts for use in downstream notebooks.\n\n**What to export:**\n1. `df_with_specialty.parquet` - Full dataframe with specialty column\n2. `focus_area_to_specialty.json` - Direct mapping for quick lookup\n3. `taxonomy_metadata.json` - Document all decisions and parameters\n4. Summary README\n\n**Hints:**\n- Include clustering parameters in metadata\n- Note any manual merging decisions\n- Record date and model versions"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO 13: Export final taxonomy and metadata\n\n# 1. Create focus_area \u2192 specialty mapping\n# focus_area_to_specialty = catalog_df.set_index('focus_area')['specialty'].to_dict()\n# with open(ARTIFACTS_DIR / 'focus_area_to_specialty.json', 'w') as f:\n#     json.dump(focus_area_to_specialty, f, indent=2)\n\n# 2. Create metadata\n# from datetime import datetime\n# metadata = {\n#     'created_date': datetime.now().isoformat(),\n#     'input_data': str(INPUT_PATH),\n#     'n_original_focus_areas': len(catalog),\n#     'n_final_specialties': len(df['specialty'].unique()),\n#     'total_samples': len(df),\n#     'embedding_model': 'pritamdeka/BioBERT-mnli-snli-scitail-mednli',\n#     'clustering_method': 'kmeans',  # or 'hdbscan'\n#     'clustering_params': {\n#         'n_clusters': 40,  # if kmeans\n#         # 'min_cluster_size': 40,  # if hdbscan\n#     },\n#     'specialty_list': sorted(df['specialty'].unique()),\n#     'per_specialty_counts': df['specialty'].value_counts().to_dict(),\n#     'manual_decisions': [\n#         'Merged all cancer types into Oncology',\n#         'Grouped heart conditions into Cardiology',\n#         # Add your decisions here\n#     ]\n# }\n# with open(ARTIFACTS_DIR / 'taxonomy_metadata.json', 'w') as f:\n#     json.dump(metadata, f, indent=2)\n\n# 3. Create summary README\n# readme_text = f\"\"\"\n# # Medical Specialty Taxonomy\n\n# **Created:** {datetime.now().strftime('%Y-%m-%d')}\n\n# ## Summary\n\n# - Original focus_areas: {len(catalog)}\n# - Final specialties: {len(df['specialty'].unique())}\n# - Total samples: {len(df)}\n\n# ## Method\n\n# 1. Embedded focus_area strings using BioBERT\n# 2. Clustered with k-means (K=40)\n# 3. Manually labeled clusters as medical specialties\n# 4. Validated with zero-shot classification\n# 5. Merged to full dataframe\n\n# ## Files\n\n# - `df_with_specialty.parquet` - Full dataframe with specialty column\n# - `focus_area_to_specialty.json` - Direct mapping\n# - `taxonomy_metadata.json` - Detailed metadata\n# - `catalog_with_specialty.csv` - Catalog with cluster assignments\n\n# ## Specialties\n\n# {df['specialty'].value_counts().to_string()}\n# \"\"\"\n# with open(ARTIFACTS_DIR / 'README.md', 'w') as f:\n#     f.write(readme_text)\n\n# print(\"\u2705 Exported all artifacts to:\", ARTIFACTS_DIR)\n# print(\"\\nFiles created:\")\n# print(\"  - df_with_specialty.parquet\")\n# print(\"  - focus_area_to_specialty.json\")\n# print(\"  - taxonomy_metadata.json\")\n# print(\"  - README.md\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83e\udd14 Reflection\n\nAnswer these questions after completing the taxonomy:\n\n1. **How many final specialties did you create? Is this reasonable?**\n\n2. **What were the hardest clustering decisions? Why?**\n\n3. **How did zero-shot validation help (if you ran it)?**\n\n4. **What would you do differently if starting over?**\n\n5. **Are there any specialties you're uncertain about?**\n\n6. **What did you learn about medical domain knowledge from this exercise?**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Your reflections:**\n\n*Write your answers here*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udccc Summary\n\n\u2705 Built data-driven specialty taxonomy  \n\u2705 Reduced 5,126 focus_areas \u2192 15-20 specialties  \n\u2705 Each specialty has sufficient samples  \n\u2705 Validated with zero-shot classification  \n\u2705 Exported artifacts for downstream use\n\n**Next:** `01_project_scope_and_data.ipynb` (now with manageable specialty categories!)\n\n---\n\n## \ud83c\udf93 What You Learned\n\n**NLP Skills:**\n- Semantic embeddings with BioBERT\n- Text clustering (HDBSCAN, k-means)\n- Zero-shot classification for validation\n\n**Data Science Skills:**\n- Bottom-up taxonomy construction\n- Handling extreme class imbalance (5,126 classes!)\n- Dimensionality reduction (UMAP)\n- Cluster inspection and labeling\n\n**Domain Skills:**\n- Medical specialty categorization\n- Semantic grouping of conditions\n- Balancing granularity vs. data availability\n\n**Project Management:**\n- Artifact management\n- Reproducible workflows\n- Documentation and metadata\n\n---\n\n*This notebook demonstrates advanced NLP techniques for real-world data challenges. The approach is generalizable to any multi-class problem with too many fine-grained labels!*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}