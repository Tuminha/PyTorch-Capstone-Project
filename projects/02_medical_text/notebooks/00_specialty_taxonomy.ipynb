{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß≠ From Raw Q&A to Specialty Taxonomy (Unsupervised ‚Üí Minimal Hybrid)\n",
    "\n",
    "## üéØ Goals\n",
    "\n",
    "Create a first-pass specialty label (y) for each record using an **ML-first approach**:\n",
    "\n",
    "1. **Build text representations** (embeddings)\n",
    "2. **Explore structure** (UMAP visualization)\n",
    "3. **Learn k-means clustering** and choose k with evidence\n",
    "4. **Manually name clusters** ‚Üí preliminary taxonomy\n",
    "5. **Add 5‚Äì10 surgical rules** for obvious fixes (hybrid)\n",
    "6. **Export a clean dataset** with specialty for downstream supervised learning\n",
    "\n",
    "### Why this design?\n",
    "\n",
    "You're learning **clustering**. Rules are allowed only to patch systematic misses, not to replace ML discovery.\n",
    "\n",
    "### üì¶ Deliverables\n",
    "\n",
    "- `data/processed/specialty_taxonomy_v1.csv` (includes specialty)\n",
    "- `artifacts/cluster_label_map_v1.json` (cluster ‚Üí human label)\n",
    "- `artifacts/umap_2d.parquet` (2D coords for plots)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Decisions Log\n",
    "\n",
    "**Document your key decisions here as you work through the notebook:**\n",
    "\n",
    "### Embedding Model Choice\n",
    "- **Model:** _[TODO: Fill this in after completing Section 3]_\n",
    "- **Why:** _[Explain your reasoning: medical vs. general, speed vs. quality]_\n",
    "\n",
    "### BEST_K Choice\n",
    "- **k =** _[TODO: Fill this in after completing Section 4]_\n",
    "- **Evidence:** _[1-2 paragraphs: What did the elbow curve show? What was the silhouette score? Why did you choose this k?]_\n",
    "\n",
    "### Surgical Rules (5-10 max)\n",
    "_[TODO: Fill this in after completing Section 9]_\n",
    "\n",
    "1. **Rule 1:** _[What pattern? ‚Üí Which specialty? Why needed?]_\n",
    "2. **Rule 2:** _[...]_\n",
    "3. ...\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0Ô∏è‚É£ Imports, Config, Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Imports\n",
    "# Hint: pandas, numpy, pathlib, json, matplotlib, sklearn (KMeans, silhouette_score), \n",
    "#       umap-learn, sentence-transformers\n",
    "\n",
    "\n",
    "\n",
    "# [TODO] Set a reproducible seed (e.g., 42) for KMeans and UMAP\n",
    "# Hint: np.random.seed, random.seed, pass random_state=SEED in models\n",
    "\n",
    "\n",
    "\n",
    "# [TODO] Define paths (Pathlib recommended)\n",
    "# DATA_RAW = Path(\"../../../datasets/medquad.csv\")\n",
    "# DATA_OUT = Path(\"../data/processed\")\n",
    "# ARTIFACTS = Path(\"../artifacts\")\n",
    "# Ensure directories exist (mkdir parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Load Data\n",
    "\n",
    "We'll build one text unit per record. Best default for specialty inference is **question + answer concatenated** (more context). If you only have one field, just use it.\n",
    "\n",
    "### Assumptions (edit as needed):\n",
    "- Columns available: `focus_area`, `question`, `answer`\n",
    "- Final text column: `text` (we'll create this by combining question + answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Load CSV\n",
    "# df = pd.read_csv(DATA_RAW)\n",
    "\n",
    "\n",
    "# [TODO] Sanity check columns and row count\n",
    "# print(df.columns); df.head()\n",
    "\n",
    "\n",
    "# [TODO] Build the `text` column.\n",
    "# Hint: Prefer question + \" \" + answer; fall back if a column is missing.\n",
    "# df[\"text\"] = ...\n",
    "\n",
    "\n",
    "# [TODO] Drop completely empty/NaN texts and trim whitespace\n",
    "# df = df[~df[\"text\"].isna()].copy()\n",
    "# df[\"text\"] = df[\"text\"].str.strip()\n",
    "\n",
    "\n",
    "# [TODO] Optional: keep only essential columns or add an ID if missing\n",
    "# If no ID exists: df['id'] = range(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Light Preprocessing\n",
    "\n",
    "Transformers don't need heavy cleaning. Keep it minimal:\n",
    "\n",
    "- ‚úÖ Normalize whitespace\n",
    "- ‚ùì Lowercasing (optional - embeddings handle case)\n",
    "- ‚ùå No stemming/lemmatization\n",
    "- ‚ùå Keep domain terms (don't strip \"diabetes\", \"TNM\", etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Minimal normalization step(s)\n",
    "# Hint: df[\"text_proc\"] = df[\"text\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "# Optional: .str.lower() if you want\n",
    "# Decide and document it in a variable, e.g., USE_LOWER = False\n",
    "\n",
    "\n",
    "\n",
    "# Print a few samples to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Embeddings\n",
    "\n",
    "Use a **sentence embedding model**. Options:\n",
    "\n",
    "### Medical-leaning models:\n",
    "- `pritamdeka/BioBERT-mnli-snli-scitail-mednli` (best for medical text)\n",
    "- `dmis-lab/biobert-base-cased-v1.1` (needs sentence-transformers wrapper)\n",
    "\n",
    "### General baseline models:\n",
    "- `sentence-transformers/all-MiniLM-L6-v2` (fast, decent, 384-dim)\n",
    "- `sentence-transformers/all-mpnet-base-v2` (slower, better, 768-dim)\n",
    "\n",
    "### Trade-off:\n",
    "- **Medical models** = better semantics for clinical terms\n",
    "- **General model** = faster & lighter\n",
    "\n",
    "**Pick one and state why in the Decisions Log above!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Load a SentenceTransformer model by name\n",
    "# Hint: from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer(\"<model-name>\")\n",
    "\n",
    "\n",
    "# [TODO] Compute embeddings for df[\"text_proc\"] (or df[\"text\"])\n",
    "# X = model.encode(..., batch_size=32, show_progress_bar=True, \n",
    "#                  convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "\n",
    "# [TODO] Sanity check shape: (n_samples, embedding_dim)\n",
    "# print(f\"Embeddings shape: {X.shape}\")\n",
    "# Expected: (16398, 384) or (16398, 768) depending on model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Picking k: Evidence over Vibes\n",
    "\n",
    "You'll learn k-means by comparing k values:\n",
    "\n",
    "### Two Key Metrics:\n",
    "\n",
    "1. **Inertia (distortion):** Sum of squared distances to cluster centers\n",
    "   - Goes down with higher k (use elbow to find diminishing returns)\n",
    "   - Look for the \"elbow\" where improvement flattens\n",
    "\n",
    "2. **Silhouette score:** Measures cluster separation quality\n",
    "   - Range: [-1, 1], higher is better\n",
    "   - > 0.5 = good separation\n",
    "   - 0.3-0.5 = reasonable\n",
    "   - < 0.3 = poor/overlapping clusters\n",
    "\n",
    "### Strategy:\n",
    "Try k in `[8, 10, 12, 14, 16, 18, 20, 22, 24, 26]` (adjust for dataset size). \n",
    "\n",
    "Choose a **justified k** (not perfect, just argued with evidence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Define K_RANGE = list(range(8, 27, 2))  # example: step by 2\n",
    "\n",
    "\n",
    "# [TODO] For each k:\n",
    "#   - fit KMeans(n_clusters=k, n_init=\"auto\" or 10+, random_state=SEED)\n",
    "#   - record inertia_\n",
    "#   - compute silhouette_score(X, labels)  # may be slow on huge data; sample if needed\n",
    "# Hint: Store results in two lists: inertias = [], silhouettes = []\n",
    "\n",
    "\n",
    "\n",
    "# [TODO] Plot k vs inertia (elbow curve)\n",
    "# Hint: plt.figure(figsize=(12,5))\n",
    "#       plt.subplot(1,2,1) for inertia\n",
    "#       plt.plot(K_RANGE, inertias, marker='o')\n",
    "#       plt.xlabel('k'), plt.ylabel('Inertia'), plt.title('Elbow Curve')\n",
    "\n",
    "\n",
    "# [TODO] Plot k vs silhouette score\n",
    "#       plt.subplot(1,2,2) for silhouette\n",
    "#       plt.plot(K_RANGE, silhouettes, marker='o', color='orange')\n",
    "#       plt.xlabel('k'), plt.ylabel('Silhouette'), plt.title('Silhouette Score')\n",
    "#       plt.tight_layout(), plt.show()\n",
    "\n",
    "\n",
    "# [TODO] Based on these plots, set BEST_K = ...\n",
    "# BEST_K = ???\n",
    "# print(f\"‚úÖ Chosen k = {BEST_K}\")\n",
    "# Now go back to the Decisions Log and document WHY you chose this k!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Train Final KMeans\n",
    "\n",
    "Now fit the final model with your chosen `BEST_K`. This is the **unsupervised \"discovery\" step** that gives you cluster IDs.\n",
    "\n",
    "### What KMeans Does:\n",
    "1. Randomly initialize k cluster centers\n",
    "2. Assign each point to nearest center\n",
    "3. Move centers to mean of assigned points\n",
    "4. Repeat steps 2-3 until convergence\n",
    "5. Output: cluster_id for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Fit final KMeans with BEST_K\n",
    "# km = KMeans(n_clusters=BEST_K, n_init=10, random_state=SEED, verbose=1)\n",
    "# cluster_ids = km.fit_predict(X)\n",
    "\n",
    "\n",
    "# [TODO] Attach cluster_ids to df\n",
    "# df[\"cluster_id\"] = cluster_ids\n",
    "\n",
    "\n",
    "# [TODO] Check cluster size distribution\n",
    "# df[\"cluster_id\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Visualize Structure with UMAP\n",
    "\n",
    "**Visualization is for your brain, not the algorithm.** \n",
    "\n",
    "Use UMAP (2D) to inspect the structure and color by cluster. This won't change labels; it helps you **understand what k-means found**.\n",
    "\n",
    "### UMAP Parameters:\n",
    "- `n_neighbors`: Local vs. global structure (15 is a good default)\n",
    "- `min_dist`: How tightly to pack points (0.1 is standard)\n",
    "- `metric`: Use \"cosine\" for text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Fit UMAP on X ‚Üí 2D\n",
    "# Hint: import umap\n",
    "# umap_model = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, \n",
    "#                        metric=\"cosine\", random_state=SEED, verbose=True)\n",
    "# Z = umap_model.fit_transform(X)\n",
    "\n",
    "\n",
    "# [TODO] Save Z to artifacts for reuse\n",
    "# Convert Z to a DataFrame with id, cluster_id, x, y and save as parquet\n",
    "# umap_df = pd.DataFrame({\n",
    "#     'id': df['id'],\n",
    "#     'cluster_id': df['cluster_id'],\n",
    "#     'umap_x': Z[:, 0],\n",
    "#     'umap_y': Z[:, 1]\n",
    "# })\n",
    "# umap_df.to_parquet(ARTIFACTS / \"umap_2d.parquet\", index=False)\n",
    "\n",
    "\n",
    "# [TODO] Simple scatter plot: x vs y, colored by cluster_id\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# scatter = plt.scatter(Z[:, 0], Z[:, 1], c=df['cluster_id'], \n",
    "#                       cmap='tab20', s=5, alpha=0.6)\n",
    "# plt.colorbar(scatter, label='Cluster ID')\n",
    "# plt.xlabel('UMAP 1'), plt.ylabel('UMAP 2')\n",
    "# plt.title(f'UMAP Projection (k={BEST_K} clusters)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Understand Clusters: Representatives & Keywords\n",
    "\n",
    "Before naming clusters, explore what's inside them:\n",
    "\n",
    "### Two Techniques:\n",
    "\n",
    "1. **Representatives:** Show 10 sample texts per cluster\n",
    "   - Gives you a \"feel\" for what the cluster contains\n",
    "   - Look for common themes\n",
    "\n",
    "2. **Keywords per cluster:** Compute TF-IDF on texts within each cluster\n",
    "   - Surfaces the most distinctive terms\n",
    "   - This is descriptive analysis, not the clustering itself\n",
    "\n",
    "You'll use these to **manually map cluster ‚Üí specialty**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] For each cluster_id:\n",
    "#   - print count\n",
    "#   - show N (e.g., 10) representative texts (shortened to first 200 chars)\n",
    "# Hint: \n",
    "# for cluster_id in sorted(df['cluster_id'].unique()):\n",
    "#     cluster_df = df[df['cluster_id'] == cluster_id]\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"Cluster {cluster_id} | Size: {len(cluster_df)}\")\n",
    "#     print(f\"{'='*80}\")\n",
    "#     samples = cluster_df.sample(n=min(10, len(cluster_df)), random_state=SEED)\n",
    "#     for idx, row in samples.iterrows():\n",
    "#         print(f\"  ‚Ä¢ {row['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Compute cluster-wise TF-IDF keywords\n",
    "# For each cluster, fit a TF-IDF on its texts and extract top N terms\n",
    "# Hint: \n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# \n",
    "# for cluster_id in sorted(df['cluster_id'].unique()):\n",
    "#     cluster_texts = df[df['cluster_id'] == cluster_id]['text_proc'].tolist()\n",
    "#     \n",
    "#     if len(cluster_texts) < 5:  # Skip tiny clusters\n",
    "#         continue\n",
    "#     \n",
    "#     vectorizer = TfidfVectorizer(max_features=20, ngram_range=(1,2), \n",
    "#                                   stop_words='english', min_df=2)\n",
    "#     tfidf_matrix = vectorizer.fit_transform(cluster_texts)\n",
    "#     feature_names = vectorizer.get_feature_names_out()\n",
    "#     \n",
    "#     # Get top terms\n",
    "#     scores = tfidf_matrix.sum(axis=0).A1\n",
    "#     top_indices = scores.argsort()[-15:][::-1]\n",
    "#     top_terms = [feature_names[i] for i in top_indices]\n",
    "#     \n",
    "#     print(f\"\\nCluster {cluster_id} keywords: {', '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Manual Cluster Naming\n",
    "\n",
    "Create a mapping from `cluster_id ‚Üí specialty` (your taxonomy). \n",
    "\n",
    "### Guidelines:\n",
    "- Keep names consistent (e.g., \"Oncology\", \"Endocrinology & Diabetes\", \"Neurology & Neurosurgery\", \"Cardiology\")\n",
    "- Don't overfit‚Äî**broad buckets are fine**\n",
    "- Use the representatives and keywords from Section 7 to guide your choices\n",
    "- If a cluster is truly mixed, pick the dominant theme or use \"General Medicine\"\n",
    "\n",
    "### Suggested Specialty Names:\n",
    "- Oncology\n",
    "- Infectious Diseases\n",
    "- Cardiology & Vascular\n",
    "- Neurology & Neurosurgery\n",
    "- Endocrinology & Diabetes\n",
    "- Gastroenterology & Hepatology\n",
    "- Nephrology & Urology\n",
    "- Pulmonology & Respiratory\n",
    "- Immunology & Rheumatology\n",
    "- Hematology\n",
    "- Dermatology\n",
    "- Ophthalmology\n",
    "- Obstetrics & Gynecology\n",
    "- Pediatrics & Development\n",
    "- Genetics & Rare Disorders\n",
    "- General & Preventive Medicine\n",
    "\n",
    "Save your mapping to JSON so it's **reproducible**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Create a dict: cluster_label_map = {0: \"Oncology\", 1: \"Cardiology\", ...}\n",
    "# Start empty and fill as you inspect exploration outputs from Section 7\n",
    "\n",
    "cluster_label_map = {\n",
    "    # Example:\n",
    "    # 0: \"Oncology\",\n",
    "    # 1: \"Infectious Diseases\",\n",
    "    # 2: \"Cardiology & Vascular\",\n",
    "    # ... fill in all clusters\n",
    "}\n",
    "\n",
    "# [TODO] Validate: all cluster_ids present have a mapping\n",
    "# missing = set(df[\"cluster_id\"].unique()) - set(cluster_label_map.keys())\n",
    "# if missing:\n",
    "#     raise ValueError(f\"Missing mappings for clusters: {missing}\")\n",
    "\n",
    "\n",
    "# [TODO] Apply mapping\n",
    "# df[\"specialty\"] = df[\"cluster_id\"].map(cluster_label_map)\n",
    "\n",
    "\n",
    "# [TODO] Save mapping to artifacts as JSON\n",
    "# with open(ARTIFACTS / \"cluster_label_map_v1.json\", \"w\") as f:\n",
    "#     json.dump(cluster_label_map, f, indent=2, sort_keys=True)\n",
    "# print(\"‚úÖ Saved cluster_label_map_v1.json\")\n",
    "\n",
    "\n",
    "# [TODO] Check distribution\n",
    "# df[\"specialty\"].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Minimal Hybrid Patches (Strictly 5‚Äì10 Rules)\n",
    "\n",
    "Now add **only a handful of surgical overrides** for systematic misses you observed.\n",
    "\n",
    "### Philosophy:\n",
    "- **Rules are a band-aid, not a cast**\n",
    "- If you feel tempted to add rule #11, revisit clustering (k, embeddings) instead\n",
    "- Keep them transparent and minimal‚Äîthis is still an **ML-first taxonomy**\n",
    "\n",
    "### Examples of Categories Worth Patches:\n",
    "1. `\"cancer\" / \"carcinoma\" / \"sarcoma\"` ‚Üí **Oncology**\n",
    "2. `\"diabetes\" / \"hyperglycemia\" / \"insulin\"` ‚Üí **Endocrinology & Diabetes**\n",
    "3. `\"pregnancy\" / \"obstetric\" / \"antenatal\"` ‚Üí **Obstetrics & Gynecology**\n",
    "4. `\"stroke\" / \"epilepsy\" / \"parkinson\"` ‚Üí **Neurology & Neurosurgery**\n",
    "5. `\"heart\" / \"coronary\" / \"cardiac\"` ‚Üí **Cardiology & Vascular**\n",
    "\n",
    "**Pick only those that your clusters consistently confuse.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Define a small function override_specialty(text, current_label) -> label\n",
    "# Hints:\n",
    "# - Work on a lowercased version of text\n",
    "# - Evaluate 5‚Äì10 if/elif checks MAX\n",
    "# - Return either current_label (no change) or an override\n",
    "# - Document each rule with a comment explaining WHY\n",
    "\n",
    "def override_specialty(text: str, current_label: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply minimal surgical overrides for systematic clustering misses.\n",
    "    \n",
    "    Args:\n",
    "        text: The original text (question + answer)\n",
    "        current_label: The specialty assigned by clustering\n",
    "    \n",
    "    Returns:\n",
    "        Either current_label (no override) or corrected specialty\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Rule 1: Cancer terms ‚Üí Oncology\n",
    "    # Why: Clustering sometimes groups rare cancers with genetic disorders\n",
    "    # if any(term in text_lower for term in [\"cancer\", \"carcinoma\", \"sarcoma\", \"tumor\", \"tumour\"]):\n",
    "    #     return \"Oncology\"\n",
    "    \n",
    "    # Rule 2: Diabetes ‚Üí Endocrinology\n",
    "    # Why: \"diabetes\" sometimes clusters with nephrology (kidney complications)\n",
    "    # if \"diabetes\" in text_lower or \"diabetic\" in text_lower:\n",
    "    #     return \"Endocrinology & Diabetes\"\n",
    "    \n",
    "    # [TODO] Add 3-8 more rules based on YOUR clustering results\n",
    "    # Only add rules if you observe systematic errors!\n",
    "    \n",
    "    # No override matched ‚Üí keep original\n",
    "    return current_label\n",
    "\n",
    "\n",
    "# [TODO] Apply to create specialty_v1_hybrid\n",
    "# df[\"specialty_v1\"] = df.apply(lambda r: override_specialty(r[\"text\"], r[\"specialty\"]), axis=1)\n",
    "\n",
    "\n",
    "# [TODO] Inspect how many labels changed\n",
    "# n_changed = (df[\"specialty_v1\"] != df[\"specialty\"]).sum()\n",
    "# pct_changed = (df[\"specialty_v1\"] != df[\"specialty\"]).mean() * 100\n",
    "# print(f\"\\n‚úÖ Overrides applied: {n_changed:,} labels changed ({pct_changed:.2f}%)\")\n",
    "\n",
    "\n",
    "# [TODO] Show examples of what changed\n",
    "# changed_df = df[df[\"specialty_v1\"] != df[\"specialty\"]][[\"text\", \"specialty\", \"specialty_v1\"]]\n",
    "# print(\"\\nExample overrides:\")\n",
    "# display(changed_df.sample(min(10, len(changed_df)), random_state=SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Quality Checks & Documentation\n",
    "\n",
    "Quick sanity checks before exporting:\n",
    "\n",
    "1. **Label distribution** (counts per specialty)\n",
    "2. **Random samples** per specialty to eyeball correctness\n",
    "3. **Silhouette score** for final BEST_K (already computed) ‚Üí document value\n",
    "4. **Write down** why you chose BEST_K (in Decisions Log at top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Value counts - check for balance and reasonableness\n",
    "# specialty_counts = df[\"specialty_v1\"].value_counts().sort_values(ascending=False)\n",
    "# print(\"\\nüìä Final Specialty Distribution:\")\n",
    "# print(specialty_counts)\n",
    "# print(f\"\\nTotal specialties: {len(specialty_counts)}\")\n",
    "# print(f\"Largest: {specialty_counts.iloc[0]} samples\")\n",
    "# print(f\"Smallest: {specialty_counts.iloc[-1]} samples\")\n",
    "# print(f\"Imbalance ratio: {specialty_counts.iloc[0] / specialty_counts.iloc[-1]:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Random samples per specialty for eyeballing\n",
    "# Sample 3-5 texts from each specialty and verify they make sense\n",
    "\n",
    "# print(\"\\nüîç Quality Check: Random Samples per Specialty\")\n",
    "# print(\"=\"*80)\n",
    "# for specialty in sorted(df[\"specialty_v1\"].unique()):\n",
    "#     print(f\"\\nüìã {specialty}\")\n",
    "#     print(\"-\"*80)\n",
    "#     samples = df[df[\"specialty_v1\"] == specialty].sample(\n",
    "#         n=min(3, len(df[df[\"specialty_v1\"] == specialty])), \n",
    "#         random_state=SEED\n",
    "#     )\n",
    "#     for idx, row in samples.iterrows():\n",
    "#         print(f\"  ‚Ä¢ {row['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Save Outputs\n",
    "\n",
    "These files are your **contract with downstream supervised notebooks**:\n",
    "\n",
    "- `data/processed/specialty_taxonomy_v1.csv` (must contain: id, text, specialty)\n",
    "- `artifacts/cluster_label_map_v1.json` (reproducible cluster ‚Üí specialty mapping)\n",
    "- `artifacts/umap_2d.parquet` (optional but helpful for viz reuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Persist final labeled dataset\n",
    "# OUT = DATA_OUT / \"specialty_taxonomy_v1.csv\"\n",
    "# df_out = df[[\"id\", \"text\"]].copy()\n",
    "# df_out[\"specialty\"] = df[\"specialty_v1\"]\n",
    "# df_out.to_csv(OUT, index=False)\n",
    "# print(f\"‚úÖ Saved: {OUT}\")\n",
    "# print(f\"   Shape: {df_out.shape}\")\n",
    "# print(f\"   Columns: {list(df_out.columns)}\")\n",
    "\n",
    "\n",
    "# [TODO] Print all saved artifact paths for visibility\n",
    "# print(\"\\nüì¶ All Artifacts Saved:\")\n",
    "# print(f\"  1. {OUT}\")\n",
    "# print(f\"  2. {ARTIFACTS / 'cluster_label_map_v1.json'}\")\n",
    "# print(f\"  3. {ARTIFACTS / 'umap_2d.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Appendix A: K-means Intuition\n",
    "\n",
    "### What it optimizes:\n",
    "Minimizes **within-cluster squared distances** (inertia). Think: \"each cluster has a center; assign points to nearest center; move centers; repeat.\"\n",
    "\n",
    "### Why k matters:\n",
    "- **Too small** ‚Üí unrelated topics merged\n",
    "- **Too large** ‚Üí fragments one topic into many micro-clusters\n",
    "\n",
    "### Why embeddings first:\n",
    "Raw word counts miss context; embeddings place semantically similar texts closer in vector space.\n",
    "\n",
    "### Why silhouette:\n",
    "A simple measure of how well a point fits within its cluster vs. the next best cluster. Use it to avoid wishful thinking.\n",
    "\n",
    "### Why UMAP:\n",
    "Your eyes are good anomaly detectors. UMAP is for **understanding**, not for deciding labels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Appendix B: Minimal Rules Philosophy\n",
    "\n",
    "### Key Principles:\n",
    "\n",
    "1. **Rules are a band-aid, not a cast**\n",
    "   - Use them to patch systematic errors, not replace ML\n",
    "   - If you need many rules, your clustering needs improvement\n",
    "\n",
    "2. **If you feel tempted to add rule #11:**\n",
    "   - Revisit clustering parameters (k, embeddings model) instead\n",
    "   - Consider using a better embedding model\n",
    "   - Try different k values\n",
    "\n",
    "3. **Document rules in plain language:**\n",
    "   - Each rule: **what** pattern? ‚Üí **which** specialty? **why** needed?\n",
    "   - Example: \"Cancer terms ‚Üí Oncology. Why: k-means grouped rare cancers with genetic syndromes due to 'rare disease' semantic similarity.\"\n",
    "\n",
    "4. **Prefer ML improvements over rules:**\n",
    "   - Better embeddings (BioBERT vs. general model)\n",
    "   - Different k (more/fewer clusters)\n",
    "   - Alternative clustering (hierarchical, HDBSCAN)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Next Steps\n",
    "\n",
    "When you're done with this notebook:\n",
    "\n",
    "1. **Go back to Decisions Log** at the top and fill in all sections\n",
    "2. **Verify all artifacts exist:**\n",
    "   - `data/processed/specialty_taxonomy_v1.csv`\n",
    "   - `artifacts/cluster_label_map_v1.json`\n",
    "   - `artifacts/umap_2d.parquet`\n",
    "\n",
    "3. **Proceed to `01_project_scope_and_data.ipynb`:**\n",
    "   - Load `specialty_taxonomy_v1.csv`\n",
    "   - Define success metrics\n",
    "   - Split data for supervised learning\n",
    "\n",
    "4. **Later: Stress-test your taxonomy:**\n",
    "   - Train supervised classifiers (Notebook 04-06)\n",
    "   - Measure downstream F1 scores\n",
    "   - Analyze confusion matrix to see if clusters make sense\n",
    "   - **Science beats vibes!** üî¨\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Remember:** The goal is **learning**, not perfection. A taxonomy with 60-70% accuracy built through understanding is better than 95% accuracy from a black box.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
