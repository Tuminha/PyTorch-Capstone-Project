{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Evaluation & Conclusions",
    "",
    "## \ud83c\udfaf Concept Primer",
    "Honest out-of-sample evaluation. Load best model, evaluate on test set, and write conclusions.",
    "",
    "**Metrics:** Accuracy, Precision, Recall, F1, ROC-AUC  ",
    "**Visualizations:** ROC curve, PR curve, Confusion Matrix  ",
    "**Optional:** Threshold sweep for operating point selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccb Objectives",
    "1. Load best PyTorch model",
    "2. Evaluate on test set",
    "3. Compare all models (baselines vs FFN)",
    "4. Plot ROC and PR curves",
    "5. Write conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2705 Acceptance Criteria",
    "- [ ] Best model loaded and evaluated",
    "- [ ] Metrics table for all models",
    "- [ ] ROC and PR curves plotted",
    "- [ ] Confusion matrix visualized",
    "- [ ] Conclusions written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO 1: Import libraries",
    "# import torch",
    "# import torch.nn as nn",
    "# from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix",
    "# import matplotlib.pyplot as plt",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Load Best Model",
    "",
    "### TODO 2: Load saved model weights",
    "",
    "**Expected:** model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO 2: Load best model",
    "# model.load_state_dict(best_model_state)",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Test Set Evaluation",
    "",
    "### TODO 3: Evaluate on test set",
    "",
    "**Expected:** Get predictions and probabilities  ",
    "**Metrics:** Accuracy, Precision, Recall, F1, ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO 3: Test evaluation",
    "# with torch.no_grad():",
    "#     X_test_tensor = torch.FloatTensor(X_test.values)",
    "#     y_test_tensor = torch.FloatTensor(y_test.values)",
    "#     test_outputs = model(X_test_tensor).squeeze()",
    "#     test_probs = torch.sigmoid(test_outputs).numpy()",
    "#     test_preds = (test_probs > 0.5).astype(int)",
    "# ",
    "# # Compute metrics",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score",
    "# test_metrics = {",
    "#     'accuracy': accuracy_score(y_test, test_preds),",
    "#     'precision': precision_score(y_test, test_preds),",
    "#     'recall': recall_score(y_test, test_preds),",
    "#     'f1': f1_score(y_test, test_preds),",
    "#     'roc_auc': roc_auc_score(y_test, test_probs)",
    "# }",
    "# print(\"Test Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Visualizations",
    "",
    "### TODO 4: Plot ROC and PR curves",
    "",
    "**Expected:** Two plots showing model performance  ",
    "**Save:** Export to `/images/diabetes_roc.png` and `/images/diabetes_pr.png`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO 4: Plot ROC curve",
    "# fpr, tpr, _ = roc_curve(y_test, test_probs)",
    "# plt.figure(figsize=(8, 6))",
    "# plt.plot(fpr, tpr, label=f'PyTorch FFN (AUC={test_metrics[\"roc_auc\"]:.3f})')",
    "# plt.plot([0, 1], [0, 1], 'k--')",
    "# plt.xlabel('False Positive Rate')",
    "# plt.ylabel('True Positive Rate')",
    "# plt.title('ROC Curve')",
    "# plt.legend()",
    "# plt.savefig('../../images/diabetes_roc.png', dpi=150)",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Confusion Matrix",
    "",
    "### TODO 5: Visualize confusion matrix",
    "",
    "**Expected:** Heatmap showing TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO 5: Confusion matrix",
    "# cm = confusion_matrix(y_test, test_preds)",
    "# plt.figure(figsize=(8, 6))",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')",
    "# plt.xlabel('Predicted')",
    "# plt.ylabel('Actual')",
    "# plt.title('Confusion Matrix')",
    "# plt.savefig('../../images/diabetes_confusion.png', dpi=150)",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd Conclusions",
    "",
    "### TODO 6: Write final conclusions",
    "",
    "**Include:**",
    "- Key findings",
    "- Model comparison",
    "- Limitations",
    "- Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your conclusions:**",
    "",
    "*Write here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\udd14 Reflection",
    "1. Which errors matter most in diabetes screening?",
    "2. What threshold would you use in production?",
    "3. What would you do differently next time?",
    "4. How did this project advance your skills?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your reflection:**",
    "",
    "*Write here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccc Summary",
    "\u2705 All models evaluated  ",
    "\u2705 Final metrics documented  ",
    "\u2705 Conclusions written  ",
    "\u2705 Project complete!",
    "",
    "**Next:** Document in README"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}