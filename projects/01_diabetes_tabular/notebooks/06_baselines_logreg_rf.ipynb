{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models: Logistic Regression & Random Forest\n",
    "\n",
    "## ðŸŽ¯ Concept Primer\n",
    "Baselines sanity-check your preprocessing and provide a performance floor. If a neural net doesn't beat Logistic Regression, investigate why.\n",
    "\n",
    "**Model 1:** Logistic Regression (linear, interpretable)  \n",
    "**Model 2:** Random Forest (non-linear, feature importance)\n",
    "\n",
    "Expected: Train both models, evaluate on validation set, compare metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Objectives\n",
    "1. Train Logistic Regression with class weights\n",
    "2. Train Random Forest with hyperparameter tuning\n",
    "3. Evaluate both on validation set\n",
    "4. Compare metrics (Accuracy, Precision, Recall, F1, ROC-AUC)\n",
    "5. Visualize confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Acceptance Criteria\n",
    "- [ ] Logistic Regression trained and evaluated\n",
    "- [ ] Random Forest trained and evaluated\n",
    "- [ ] Metrics table comparing both models\n",
    "- [ ] Confusion matrices plotted\n",
    "- [ ] Best baseline identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../../datasets/diabetes_BRFSS2015.csv\")\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "numeric_cols = ['bmi', 'genhlth', 'menthlth', 'physhlth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "with open('../preprocessed_data/preprocessed_train_test_val.pkl', 'rb') as f:  # notice 'rb' for read\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "X_train = data_dict['X_train']\n",
    "X_val = data_dict['X_val']\n",
    "X_test = data_dict['X_test']\n",
    "y_train = data_dict['y_train']\n",
    "y_val = data_dict['y_val']\n",
    "y_test = data_dict['y_test']\n",
    "class_weights = data_dict['class_weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Logistic Regression Baseline\n",
    "\n",
    "### TODO 2: Train Logistic Regression\n",
    "\n",
    "**Parameters:** Use class_weight='balanced' to handle imbalance  \n",
    "**Expected:** Fit on X_train, y_train; predict on X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6439346157889204\n",
      "0.52827709450226\n"
     ]
    }
   ],
   "source": [
    "# TODO 2: Train Logistic Regression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_val)\n",
    "y_proba_lr = lr.predict_proba(X_val)\n",
    "score_lr = accuracy_score(y_true=y_test, y_pred=y_pred_lr)\n",
    "print(lr.score(X_val, y_val))\n",
    "print(score_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ² Random Forest Baseline\n",
    "\n",
    "### TODO 3: Train Random Forest\n",
    "\n",
    "**Parameters:** n_estimators=100, max_depth=10, class_weight='balanced'  \n",
    "**Expected:** Fit on X_train, y_train; predict on X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6790444654683065\n",
      "0.5594712498686009\n"
     ]
    }
   ],
   "source": [
    "# TODO 3: Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "y_proba_rf = rf.predict_proba(X_val)\n",
    "score_rf = accuracy_score(y_true=y_test, y_pred=y_pred_rf)\n",
    "\n",
    "print(rf.score(X_val, y_val))\n",
    "print(score_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Evaluate Baselines\n",
    "\n",
    "### TODO 4: Compute metrics for both models\n",
    "\n",
    "**Metrics:** Accuracy, Precision, Recall, F1, ROC-AUC  \n",
    "**Use:** classification_report and roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: {'accuracy': 0.6439346157889204, 'precision_weighted': 0.8521270719498015, 'recall_weighted': 0.6439346157889204, 'f1_weighted': 0.719372702594614, 'f1_macro': 0.4270835418115904, 'roc_auc_ovr_weighted': 0.8153886287972445}\n",
      "Random Forest: {'accuracy': 0.6790444654683065, 'precision_weighted': 0.8431263669167273, 'recall_weighted': 0.6790444654683065, 'f1_weighted': 0.7336212036910527, 'f1_macro': 0.4288505468778819, 'roc_auc_ovr_weighted': 0.8155788248246245}\n",
      "Confusion Matrix LR\n",
      "[[21133  5532  5391]\n",
      " [  175   218   301]\n",
      " [  893  1257  3152]]\n",
      "\n",
      "Confusion Matrix RF\n",
      "[[22031  2732  7293]\n",
      " [  215    92   387]\n",
      " [ 1035   551  3716]]\n"
     ]
    }
   ],
   "source": [
    "# TODO 4: Evaluate models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "# \n",
    "metrics_lr = {\n",
    "    'accuracy': accuracy_score(y_val, y_pred_lr),\n",
    "    'precision_weighted': precision_score(y_val, y_pred_lr, average='weighted'),\n",
    "    'recall_weighted': recall_score(y_val, y_pred_lr, average='weighted'),\n",
    "    'f1_weighted': f1_score(y_val, y_pred_lr, average='weighted'),\n",
    "    'f1_macro': f1_score(y_val, y_pred_lr, average='macro'),  # Add this too!\n",
    "    'roc_auc_ovr_weighted': roc_auc_score(y_val, y_proba_lr, multi_class='ovr', average='weighted')\n",
    "}\n",
    "\n",
    "metrics_rf = {\n",
    "    'accuracy': accuracy_score(y_val, y_pred_rf),\n",
    "    'precision_weighted': precision_score(y_val, y_pred_rf, average='weighted'),  # Add average!\n",
    "    'recall_weighted': recall_score(y_val, y_pred_rf, average='weighted'),        # Add average!\n",
    "    'f1_weighted': f1_score(y_val, y_pred_rf, average='weighted'),                # Add average!\n",
    "    'f1_macro': f1_score(y_val, y_pred_rf, average='macro'),                      # Add macro F1!\n",
    "    'roc_auc_ovr_weighted': roc_auc_score(y_val, y_proba_rf, multi_class='ovr', average='weighted')  # Fix ROC-AUC!\n",
    "}\n",
    "# \n",
    "print(\"Logistic Regression:\", metrics_lr)\n",
    "print(\"Random Forest:\", metrics_rf)\n",
    "\n",
    "print(\"Confusion Matrix LR\")\n",
    "print(confusion_matrix(y_val, y_pred_lr))\n",
    "print()\n",
    "print(\"Confusion Matrix RF\")\n",
    "print(confusion_matrix(y_val, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Baseline results saved successfully!\n",
      "   - Metrics: ../baseline_results/baseline_metrics.json\n",
      "   - LR Model: ../baseline_results/logistic_regression_model.pkl\n",
      "   - RF Model: ../baseline_results/random_forest_model.pkl\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "   Logistic Regression - Accuracy: 0.6439, F1 Macro: 0.4271\n",
      "   Random Forest       - Accuracy: 0.6790, F1 Macro: 0.4289\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('../baseline_results', exist_ok=True)\n",
    "\n",
    "# 1. Prepare baseline results dictionary\n",
    "baseline_results = {\n",
    "    'logistic_regression': {\n",
    "        'metrics': metrics_lr,\n",
    "        'confusion_matrix': confusion_matrix(y_val, y_pred_lr).tolist(),\n",
    "        'classification_report': classification_report(y_val, y_pred_lr, \n",
    "                                                       target_names=['No Diabetes', 'Prediabetes', 'Diabetes'],\n",
    "                                                       output_dict=True)\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'metrics': metrics_rf,\n",
    "        'confusion_matrix': confusion_matrix(y_val, y_pred_rf).tolist(),\n",
    "        'classification_report': classification_report(y_val, y_pred_rf,\n",
    "                                                       target_names=['No Diabetes', 'Prediabetes', 'Diabetes'],\n",
    "                                                       output_dict=True)\n",
    "    },\n",
    "    'metadata': {\n",
    "        'dataset': 'BRFSS 2015 Diabetes',\n",
    "        'validation_set_size': len(y_val),\n",
    "        'class_distribution': {\n",
    "            'class_0': int((y_val == 0).sum()),\n",
    "            'class_1': int((y_val == 1).sum()),\n",
    "            'class_2': int((y_val == 2).sum())\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. Save as JSON (human-readable)\n",
    "with open('../baseline_results/baseline_metrics.json', 'w') as f:\n",
    "    json.dump(baseline_results, f, indent=2)\n",
    "\n",
    "# 3. Save trained models (for potential reuse)\n",
    "import joblib\n",
    "joblib.dump(lr, '../baseline_results/logistic_regression_model.pkl')\n",
    "joblib.dump(rf, '../baseline_results/random_forest_model.pkl')\n",
    "\n",
    "print(\"âœ… Baseline results saved successfully!\")\n",
    "print(f\"   - Metrics: ../baseline_results/baseline_metrics.json\")\n",
    "print(f\"   - LR Model: ../baseline_results/logistic_regression_model.pkl\")\n",
    "print(f\"   - RF Model: ../baseline_results/random_forest_model.pkl\")\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   Logistic Regression - Accuracy: {metrics_lr['accuracy']:.4f}, F1 Macro: {metrics_lr['f1_macro']:.4f}\")\n",
    "print(f\"   Random Forest       - Accuracy: {metrics_rf['accuracy']:.4f}, F1 Macro: {metrics_rf['f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤” Reflection\n",
    "1. Which baseline performs better? Why?\n",
    "2. What patterns do you see in confusion matrices?\n",
    "3. Are baselines good enough for your use case?\n",
    "4. What should the PyTorch model beat?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your reflection:**\n",
    "\n",
    "### Model Performance Comparison\n",
    "\n",
    "**Random Forest performs better overall:**\n",
    "- Accuracy: 67.9% vs 64.4% (LR)\n",
    "- F1 weighted: 0.734 vs 0.719 (LR)\n",
    "- Better at majority classes (No Diabetes, Diabetes)\n",
    "\n",
    "**Key Challenge: Prediabetes class (class 1)**\n",
    "- RF: Only 13% recall (92/694 correct)\n",
    "- LR: Only 31% recall (218/694 correct)\n",
    "- Both models struggle due to:\n",
    "  - Extreme rarity (2% of data)\n",
    "  - Ambiguous features (intermediate health state)\n",
    "  - Class weight of 18.26 causes instability\n",
    "\n",
    "**F1 Macro vs Weighted:**\n",
    "- F1 Weighted (0.734): Accounts for class imbalance, dominated by class 0 performance\n",
    "- F1 Macro (0.429): Treats classes equally, reveals poor prediabetes performance\n",
    "- Large gap indicates severe minority class issues\n",
    "\n",
    "**ROC-AUC:**\n",
    "- Both models ~0.815 (similar discrimination ability)\n",
    "- Suggests similar feature importance across models\n",
    "\n",
    "**Confusion Matrix Patterns:**\n",
    "- Both models good at \"No Diabetes\" (65-69% recall)\n",
    "- Both reasonable at \"Diabetes\" (59-70% recall)\n",
    "- Both fail at \"Prediabetes\" (13-31% recall)\n",
    "- RF more confident in predictions (fewer cross-class errors)\n",
    "\n",
    "**Realistic Goal for PyTorch:**\n",
    "- Target accuracy: 70-75% (not 85% - unrealistic given data quality and severe imbalance)\n",
    "- Focus on improving F1 Macro to 0.50-0.60\n",
    "- Try to improve prediabetes recall to 25-30%\n",
    "- Use focal loss or custom class weights to handle extreme imbalance\n",
    "- Neural networks may find non-linear patterns missed by linear/tree models\n",
    "\n",
    "**Overall Assessment:**\n",
    "Random Forest is the better baseline model, achieving 3.5% higher accuracy and better performance on majority classes. However, both models demonstrate the fundamental challenge of this dataset: predicting prediabetes with only 2% representation is extremely difficult. PyTorch should focus on incremental improvements rather than dramatic gains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Summary\n",
    "âœ… Baselines trained and evaluated  \n",
    "âœ… Metrics compared  \n",
    "âœ… Ready for PyTorch model\n",
    "\n",
    "**Next:** `07_pytorch_ffn_build_train.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy ML",
   "language": "python",
   "name": "codeacademy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
