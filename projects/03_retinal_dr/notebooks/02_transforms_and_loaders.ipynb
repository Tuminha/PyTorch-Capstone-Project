{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms & DataLoaders\n",
    "\n",
    "## üéØ Concept Primer\n",
    "Image preprocessing: resize, normalize, augment (train), deterministic (val/test).\n",
    "\n",
    "**Expected:** DataLoaders with shape [B, 3, H, W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Objectives\n",
    "1. Define transforms for train/val/test\n",
    "2. Create custom Dataset class\n",
    "3. Setup DataLoaders\n",
    "4. Verify batch shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Import libraries\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Define Transforms\n",
    "\n",
    "### TODO 2: Create transform pipelines\n",
    "\n",
    "**Train:** Resize, augment (flip, rotate), normalize  \n",
    "**Val/Test:** Resize, normalize only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Custom Dataset\n",
    "\n",
    "### TODO 3: Create Dataset class\n",
    "\n",
    "**Expected:** __getitem__ returns (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: Dataset class\n",
    "class RetinalDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['Image name']\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = row['Retinopathy grade']\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ DataLoaders\n",
    "\n",
    "### TODO 4: Create DataLoaders\n",
    "\n",
    "**Expected:** Batch size=32, shape [B, 3, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Batch shape: torch.Size([29, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# TODO 4: DataLoaders\n",
    "train_labels_df = pd.read_csv('../../../datasets/diabetic_retinopathy_images/groundtruths/training_labels.csv')\n",
    "train_images_folder = '../../../datasets/diabetic_retinopathy_images/images/training_images_small'\n",
    "\n",
    "train_dataset = RetinalDataset(train_labels_df, train_images_folder, train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch shape: {images.shape}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Reflection\n",
    "1. Batch shape correct?\n",
    "2. Augmentation choices?\n",
    "\n",
    "**Reflection**\n",
    "\n",
    "1. **Batch shape correct?**\n",
    "   - Yes! Each batch comes out as `[batch_size, channels, height, width] = [32, 3, 224, 224]`.\n",
    "   - The final batch is `[29, ...]` because the dataset (413 images) isn‚Äôt divisible by 32 ‚Äî normal behavior.\n",
    "\n",
    "2. **Augmentation choices?**\n",
    "   - ‚úÖ `Resize(224,224)`: matches pre-trained models (ResNet/EfficientNet expect 224√ó224).\n",
    "   - ‚úÖ `RandomHorizontalFlip`: duplicates of a retina across left/right orientation ‚Äî clinically valid, boosts data diversity.\n",
    "   - üîÅ Later (only on train split) we can add `RandomRotation`, `ColorJitter`, etc., to make the model robust to imaging conditions.\n",
    "   - ‚úÖ `ToTensor()` and `Normalize(mean,std)`: Converts to tensor and uses ImageNet stats for transfer learning.\n",
    "   - ‚ö†Ô∏è Validation/test should use **identical transformations minus augmentation** to avoid data leakage. (Already prepared with `val_transform` ‚Äî we‚Äôll use it once splits are in place.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your reflection:**\n",
    "\n",
    "*Write here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Summary\n",
    "‚úÖ Transforms defined  \n",
    "‚úÖ DataLoaders ready\n",
    "\n",
    "**Next:** `03_simple_cnn_scaffold.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy ML",
   "language": "python",
   "name": "codeacademy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
